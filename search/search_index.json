{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to dbFLow The Deployment framework for Oracle Database and APEX Applications","title":"Home"},{"location":"#welcome-to-dbflow","text":"","title":"Welcome to dbFLow"},{"location":"changelog/","text":"Generating Changelogs Configuration","title":"Changelog"},{"location":"changelog/#generating-changelogs","text":"","title":"Generating Changelogs"},{"location":"changelog/#configuration","text":"","title":"Configuration"},{"location":"concept/","text":"Concept Git Flow dbFlow is a deployment framework to deploy Oracle based projects / applications. dbFlow is roughly based on the so called Git Flow. This flow describes how releases of an application are delivered from the main development branch to the master branch, production. The following figure outlines this flow. Development mainly takes place on the develop branch. Features that are not included in the next release are developed in so-called feature branches. If a release has to be built from the current development state, the develop branch is merged into the release branch. This represents at the same time a so-called distribution boundary. This means that development can continue on the development branch and does not have to wait for the actual release. The release is first merged into the next higher branch. In our case this is the test branch. If an error is found here, it is fixed in the release branch and merged back to test. These changes also flow back into the development branch later. If the tests on the test stage are successful, the next branch is merged and the release is tested accordingly. This continues until the respective release has reached the master branch and therefore has reached production. dbFlow dbFlow supports the developers and release managers by building an artifact corresponding to the delta of a merge, meaning the changed files, which can be applied to the respective database instance. For each target branch a separate database or database container is expected. The artifacts that dbFLow generates by merging the branches can then be imported into the databases/containers. Modes dbFlow can generate two types of artifacts / supports two types of installation modes, which can be applied to the databases/containers. On the one hand this is a so-called INIT ial delivery, which empties the target schemas first, or expects empty schemas, and on the other hand a PATCH delivery, which requires the corresponding previous version of a deployment on the database. Info Usually, the initial delivery is only installed ONCE on a production system. After that, only patch versions are applied. The advantage of this two-track approach is that a version of an application can always be built directly and thus forms the basis of a subsequent patch. For example, one can build the initial version of a predecessor and install the current state as a patch on top of it. This procedure forms the basis of flawless releases. With this approach it is easy to map the concept of NighlyBuilds with Jenkins or similar CI/CD tools. To test whether a release can be applied to Prod/Master, it is sufficient to create an initial release of Master and apply it to a test database. Afterwards, the patch, in other words the delta to the development branch, is applied. Important In INIT mode, all files from all directories except those named with Patch are applied. In PATCH mode only the changed files are applied. Files inside .hook - folders will be always executed Main Folders To support this concept a dbFlow project and its directory structure must have a certain structure. dbFLow expects the following directories at the root level of the project itself. Folder Description apex All APEX application will be stored in this folder db This folder contains the required database schemas and their objects. db/_setup This folder contains the required objects your application depends on. All scripts inside are called by the preferred admin account you configured (sys, admin, ...) rest Here the REST services / modules are stored reports binary files for templating purpose static In this folder the files are stored, which you upload later with the upload command to the static files of the respective application. (managed by dbFlux ) Project Types This structure is independent from the project-type. dbFlow knows 3 types of projects: SingleSchema, MultiSchema and FlexSchema SingleSchema and MultiSchema are very similar. APEX applications and REST modules are imported or exported with the application user. The name of the workspace is stored in both modes in the project configuration. In MultiSchema mode two additional schemas (LOGIC and DATA) are used. Here a three layer model is built up, which holds the data in the DATA schema, the business logic in the LOGIC schema and everything that is application specific in the APP schema. In SingleSchema mode, a direct database connection is established with the respective schema. In MultiSchema mode, as well as in FlexSchema mode, a ProxyUser is used for the database connection, which then connects to the target schema. If the project is configured in FlexMode , you can freely configure the corresponding schemas and workspaces via the directory structure. When using FlexMode , APEX applications are stored within workspace folders, which are again located within schema folders. The same applies to the static subdirectories, of course. REST modules are also stored in their own schema folder in FlexMode . Importing or exporting is done via the proxy user, which connects to the respective schema, which is configured by directory name. Schema Folders Each schemafolder inside db folder except _setup is build with the same structur. Folder Description db .. schema .... constraints Constraints are stored here and subdivided according to their type ...... checks Check Constraints ...... foreigns Foreign Keys ...... primaries Primary Keys ...... uniques Unique Keys .... contexts Sys_Contexts when needed .... ddl DDL Scripts for deployment subddivided on deployment mode ...... init Init scripts are executed only for the first installation (init). ...... patch Patch scripts are executed only in the update case (patch) and are divided into scripts that are executed at the beginning or at the end of the respective update. ........ post ........ pre .... dml DML Scripts for deployment subddivided on deployment mode ...... base Base scripts are always executed, no matter in what mode the deployment is used (first install - init) or (update - patch). Therefore they must be restartable. ...... init Init scripts are executed only for the first installation (init). ...... patch Patch scripts are executed only in the update case (patch) and are divided into scripts that are executed at the beginning or at the end of the respective update. ........ post ........ pre .... indexes Indexes are stored here and subdivided according to their type ...... defaults Non uniqe indexes ...... primaries Unique Indexes based in primary key columns ...... uniques Unique Indexes .... jobs Jobs, Scheduler scripts goes here .... policies Policies .... sequences Sequences must be scripted in a restartable manner .... sources All PL/SQL Code is stored in respective subfolders ...... functions ...... packages Extension for package specification is pks and extension pkb is used for body ...... procedures ...... triggers ...... types .... views Views goes here .... tables Here are all create table scripts stored ...... tables_ddl All table alter or modification scripts named with tablename.num.sql goes here .....tests Unittests ...... packages Packages containing utPLSQL Unittests All files must be stored as executable SQL scripts. dbFlow uses SQLPlus or SQLcl to import these scripts. Warning Don't forget the trailing slashes in PLSQL files The file structure has several purposes. On the one hand, these directories represent a fixed point in time during delivery, i.e. they reflect the order of execution. For example, tables are applied before the indexes and the constraints. Primary key constraints are applied before foreign key constraints, and so on. In addition, the division of the table directory into tables and tables_ddl represents a kind of toggle. In case of an initial delivery only the scripts from the table directory are executed. If it is a patch delivery, the script from the table directory will not be executed if files with the same root name exist in the tables_ddl directory. For example, if the table employees changes, the file is updated accordingly with the actual create-table statement. In addition, a matching alter-table script with the same base name is stored in the tables_ddl directory. // employees . sql create table employees ( emp_id number not null , emp_name varchar2 ( 250 char ) not null , emp_birthday date -- new column ); // employees . 1 . sql alter table employees add ( emp_birthday date -- new column ); files content tables - table_ddl - - employees.1.sql alter table add (..); - employees.sql create table (..); Tip Changes to tables are always made 2 times. Once for a new installation (init) and once for an update (patch). Table scripts may only contain the pure tables and comments. Constraints and indexes must be stored in the appropriate directories. Files from the table directory are executed if it is a new installation (init) or if the table is to be newly created in this update (patch). APEX The used applications are stored in the apex directory. The applications will be expected in splitted form. For each application there is a corresponding folder containing the respective files (standard APEX export). For a deployment only the appropriate changes are included. A configuration option can be used here, in order to include all files into the deployment. \u251c\u2500 apex \u2502 \u2514\u2500 f1000 \u2502 \u2502 \u2514\u2500 application \u2502 \u2502 \u2514\u2500 install.sql \u2502 \u2514\u2500 f2000 In SingleSchema and MultiSchema mode, the workspace is stored in the project configuration. The target schema is always the APP schema. If you use FlexSchema mode, however, this information is stored in the directory tree itself. \u251c\u2500 apex \u2502 \u2514\u2500 schema_a \u2502 \u2502 \u2514\u2500 workspace_x \u2502 \u2502 \u2514\u2500 f1000 \u2502 \u2514\u2500 schema_b \u2502 \u2514\u2500 workspace_y \u2502 \u2514\u2500 f2000 REST If you use REST modules in your project, they are placed in the rest folder. Here dbFlow also expects a certain directory structure. This has a direct influence on the order of execution during the deployment. \u251c\u2500 rest \u2502 \u2514\u2500 access \u2502 \u2502 \u2514\u2500 roles \u2502 \u2502 \u2514\u2500 privileges \u2502 \u2502 \u2514\u2500 mapping \u2502 \u2514\u2500 module \u2502 \u2514\u2500 module_name_a \u2502 \u2502 \u2514\u2500 module_name_a.module.sql \u2502 \u2514\u2500 module_name_b \u2502 \u2514\u2500 module_name_b.module.sql \u2502 \u2514\u2500 module_name_b.condition.sql Files are imported into these folders in the following order. access/roles access/privileges access/mapping modules Within the folders the order is alphabetical. If dbFLow finds a module file with the same file base and the extension *.condition.sql, this condition will be included in the install script. Important In the Condtion file a PLSQL expression is expected which returns a boolean value. Later this expression becomes part of an IF condition. Hooks On each level of the directories there are so called .hooks folders. You can find them in the root directory, in the db folder and in the schema folders. The .hooks folders are always divided into the subfolders pre and post . During the deployment process the scripts there will be executed in alphabetically order. These type of folders are meant to hold scripts which won't change too much during lifecycle of your product. For example you could place some kind of generator script inside here (Gen TableAPI, ...). Note Hookscripts outside the respective schema folders must contain the corresponding target schema in the name. \u251c\u2500 .hooks \u2502 \u251c\u2500 pre \u2502 \u2502 \u2514\u2500 01_schema_a_do_something.sql \u2502 \u2502 \u2514\u2500 02_schema_a_do_something.sql \u2502 \u2514\u2500 post \u2502 \u2514\u2500 01_schema_c_do_something.sql \u2502 \u2514\u2500 02_schema_b_do_something.sql \u2502 \u2514\u2500 03_schema_b_do_something.sql \u251c\u2500 apex \u251c\u2500 db \u2502 \u251c\u2500 .hooks \u2502 \u2502 \u2514\u2500 pre \u2502 \u2502 \u2514\u2500 post \u2502 \u251c\u2500 schema_a \u2502 \u2514\u2500 schema_b \u2502 \u2514\u2500 .hooks \u2502 \u2502 \u2514\u2500 pre \u2502 \u2502 \u2514\u2500 post \u2502 \u2514\u2500 ... \u251c\u2500 rest \u251c\u2500 static To execute a hook on a specific object type, you can add this structure to the corresponding directory within the schema hook. \u251c\u2500 db \u2502 \u2514\u2500 schema_a \u2502 \u2514\u2500 .hooks \u2502 \u2502 \u2514\u2500 pre \u2502 \u2502 \u2514\u2500 post \u2502 \u2502 \u2514\u2500 sources \u2502 \u2502 \u2514\u2500 packages \u2502 \u2502 \u2514\u2500 call_me_after_packages_applied.sql \u2502 \u2514\u2500 sources \u2502 \u2514 packages \u2502 \u2514\u2500 my_package.pks \u2502 \u2514\u2500 my_package.pkb Depot When you create a deployment, whether INIT or PATCH, the deployment artifact is stored in a so-called depot. From this depot, a CI/CD tool, for example, can later fetch the artifact and install it on the target instance. Within the depot directory, the individual deployments are stored in subdirectories that correspond to the Git branch from which they were created. A deployment is stored in the ready subfolder. After a successful installation, the patch, including all logs and temporary files, is placed in the success folder. Errors during the deployment lead to an abort and are stored in the failures subfolder. I recommend to use a separate repository or directory for each stage DB. This has the advantage that the corresponding directories serve their purpose even without Git and possibly access to the development repository. Theoretically, the repository can also be \"doubled\" to have a target directory at home and a source directory at the customer.","title":"Concept"},{"location":"concept/#concept","text":"","title":"Concept"},{"location":"concept/#git-flow","text":"dbFlow is a deployment framework to deploy Oracle based projects / applications. dbFlow is roughly based on the so called Git Flow. This flow describes how releases of an application are delivered from the main development branch to the master branch, production. The following figure outlines this flow. Development mainly takes place on the develop branch. Features that are not included in the next release are developed in so-called feature branches. If a release has to be built from the current development state, the develop branch is merged into the release branch. This represents at the same time a so-called distribution boundary. This means that development can continue on the development branch and does not have to wait for the actual release. The release is first merged into the next higher branch. In our case this is the test branch. If an error is found here, it is fixed in the release branch and merged back to test. These changes also flow back into the development branch later. If the tests on the test stage are successful, the next branch is merged and the release is tested accordingly. This continues until the respective release has reached the master branch and therefore has reached production.","title":"Git Flow"},{"location":"concept/#dbflow","text":"dbFlow supports the developers and release managers by building an artifact corresponding to the delta of a merge, meaning the changed files, which can be applied to the respective database instance. For each target branch a separate database or database container is expected. The artifacts that dbFLow generates by merging the branches can then be imported into the databases/containers.","title":"dbFlow"},{"location":"concept/#modes","text":"dbFlow can generate two types of artifacts / supports two types of installation modes, which can be applied to the databases/containers. On the one hand this is a so-called INIT ial delivery, which empties the target schemas first, or expects empty schemas, and on the other hand a PATCH delivery, which requires the corresponding previous version of a deployment on the database. Info Usually, the initial delivery is only installed ONCE on a production system. After that, only patch versions are applied. The advantage of this two-track approach is that a version of an application can always be built directly and thus forms the basis of a subsequent patch. For example, one can build the initial version of a predecessor and install the current state as a patch on top of it. This procedure forms the basis of flawless releases. With this approach it is easy to map the concept of NighlyBuilds with Jenkins or similar CI/CD tools. To test whether a release can be applied to Prod/Master, it is sufficient to create an initial release of Master and apply it to a test database. Afterwards, the patch, in other words the delta to the development branch, is applied. Important In INIT mode, all files from all directories except those named with Patch are applied. In PATCH mode only the changed files are applied. Files inside .hook - folders will be always executed","title":"Modes"},{"location":"concept/#main-folders","text":"To support this concept a dbFlow project and its directory structure must have a certain structure. dbFLow expects the following directories at the root level of the project itself. Folder Description apex All APEX application will be stored in this folder db This folder contains the required database schemas and their objects. db/_setup This folder contains the required objects your application depends on. All scripts inside are called by the preferred admin account you configured (sys, admin, ...) rest Here the REST services / modules are stored reports binary files for templating purpose static In this folder the files are stored, which you upload later with the upload command to the static files of the respective application. (managed by dbFlux )","title":"Main Folders"},{"location":"concept/#project-types","text":"This structure is independent from the project-type. dbFlow knows 3 types of projects: SingleSchema, MultiSchema and FlexSchema SingleSchema and MultiSchema are very similar. APEX applications and REST modules are imported or exported with the application user. The name of the workspace is stored in both modes in the project configuration. In MultiSchema mode two additional schemas (LOGIC and DATA) are used. Here a three layer model is built up, which holds the data in the DATA schema, the business logic in the LOGIC schema and everything that is application specific in the APP schema. In SingleSchema mode, a direct database connection is established with the respective schema. In MultiSchema mode, as well as in FlexSchema mode, a ProxyUser is used for the database connection, which then connects to the target schema. If the project is configured in FlexMode , you can freely configure the corresponding schemas and workspaces via the directory structure. When using FlexMode , APEX applications are stored within workspace folders, which are again located within schema folders. The same applies to the static subdirectories, of course. REST modules are also stored in their own schema folder in FlexMode . Importing or exporting is done via the proxy user, which connects to the respective schema, which is configured by directory name.","title":"Project Types"},{"location":"concept/#schema-folders","text":"Each schemafolder inside db folder except _setup is build with the same structur. Folder Description db .. schema .... constraints Constraints are stored here and subdivided according to their type ...... checks Check Constraints ...... foreigns Foreign Keys ...... primaries Primary Keys ...... uniques Unique Keys .... contexts Sys_Contexts when needed .... ddl DDL Scripts for deployment subddivided on deployment mode ...... init Init scripts are executed only for the first installation (init). ...... patch Patch scripts are executed only in the update case (patch) and are divided into scripts that are executed at the beginning or at the end of the respective update. ........ post ........ pre .... dml DML Scripts for deployment subddivided on deployment mode ...... base Base scripts are always executed, no matter in what mode the deployment is used (first install - init) or (update - patch). Therefore they must be restartable. ...... init Init scripts are executed only for the first installation (init). ...... patch Patch scripts are executed only in the update case (patch) and are divided into scripts that are executed at the beginning or at the end of the respective update. ........ post ........ pre .... indexes Indexes are stored here and subdivided according to their type ...... defaults Non uniqe indexes ...... primaries Unique Indexes based in primary key columns ...... uniques Unique Indexes .... jobs Jobs, Scheduler scripts goes here .... policies Policies .... sequences Sequences must be scripted in a restartable manner .... sources All PL/SQL Code is stored in respective subfolders ...... functions ...... packages Extension for package specification is pks and extension pkb is used for body ...... procedures ...... triggers ...... types .... views Views goes here .... tables Here are all create table scripts stored ...... tables_ddl All table alter or modification scripts named with tablename.num.sql goes here .....tests Unittests ...... packages Packages containing utPLSQL Unittests All files must be stored as executable SQL scripts. dbFlow uses SQLPlus or SQLcl to import these scripts. Warning Don't forget the trailing slashes in PLSQL files The file structure has several purposes. On the one hand, these directories represent a fixed point in time during delivery, i.e. they reflect the order of execution. For example, tables are applied before the indexes and the constraints. Primary key constraints are applied before foreign key constraints, and so on. In addition, the division of the table directory into tables and tables_ddl represents a kind of toggle. In case of an initial delivery only the scripts from the table directory are executed. If it is a patch delivery, the script from the table directory will not be executed if files with the same root name exist in the tables_ddl directory. For example, if the table employees changes, the file is updated accordingly with the actual create-table statement. In addition, a matching alter-table script with the same base name is stored in the tables_ddl directory. // employees . sql create table employees ( emp_id number not null , emp_name varchar2 ( 250 char ) not null , emp_birthday date -- new column ); // employees . 1 . sql alter table employees add ( emp_birthday date -- new column ); files content tables - table_ddl - - employees.1.sql alter table add (..); - employees.sql create table (..); Tip Changes to tables are always made 2 times. Once for a new installation (init) and once for an update (patch). Table scripts may only contain the pure tables and comments. Constraints and indexes must be stored in the appropriate directories. Files from the table directory are executed if it is a new installation (init) or if the table is to be newly created in this update (patch).","title":"Schema Folders"},{"location":"concept/#apex","text":"The used applications are stored in the apex directory. The applications will be expected in splitted form. For each application there is a corresponding folder containing the respective files (standard APEX export). For a deployment only the appropriate changes are included. A configuration option can be used here, in order to include all files into the deployment. \u251c\u2500 apex \u2502 \u2514\u2500 f1000 \u2502 \u2502 \u2514\u2500 application \u2502 \u2502 \u2514\u2500 install.sql \u2502 \u2514\u2500 f2000 In SingleSchema and MultiSchema mode, the workspace is stored in the project configuration. The target schema is always the APP schema. If you use FlexSchema mode, however, this information is stored in the directory tree itself. \u251c\u2500 apex \u2502 \u2514\u2500 schema_a \u2502 \u2502 \u2514\u2500 workspace_x \u2502 \u2502 \u2514\u2500 f1000 \u2502 \u2514\u2500 schema_b \u2502 \u2514\u2500 workspace_y \u2502 \u2514\u2500 f2000","title":"APEX"},{"location":"concept/#rest","text":"If you use REST modules in your project, they are placed in the rest folder. Here dbFlow also expects a certain directory structure. This has a direct influence on the order of execution during the deployment. \u251c\u2500 rest \u2502 \u2514\u2500 access \u2502 \u2502 \u2514\u2500 roles \u2502 \u2502 \u2514\u2500 privileges \u2502 \u2502 \u2514\u2500 mapping \u2502 \u2514\u2500 module \u2502 \u2514\u2500 module_name_a \u2502 \u2502 \u2514\u2500 module_name_a.module.sql \u2502 \u2514\u2500 module_name_b \u2502 \u2514\u2500 module_name_b.module.sql \u2502 \u2514\u2500 module_name_b.condition.sql Files are imported into these folders in the following order. access/roles access/privileges access/mapping modules Within the folders the order is alphabetical. If dbFLow finds a module file with the same file base and the extension *.condition.sql, this condition will be included in the install script. Important In the Condtion file a PLSQL expression is expected which returns a boolean value. Later this expression becomes part of an IF condition.","title":"REST"},{"location":"concept/#hooks","text":"On each level of the directories there are so called .hooks folders. You can find them in the root directory, in the db folder and in the schema folders. The .hooks folders are always divided into the subfolders pre and post . During the deployment process the scripts there will be executed in alphabetically order. These type of folders are meant to hold scripts which won't change too much during lifecycle of your product. For example you could place some kind of generator script inside here (Gen TableAPI, ...). Note Hookscripts outside the respective schema folders must contain the corresponding target schema in the name. \u251c\u2500 .hooks \u2502 \u251c\u2500 pre \u2502 \u2502 \u2514\u2500 01_schema_a_do_something.sql \u2502 \u2502 \u2514\u2500 02_schema_a_do_something.sql \u2502 \u2514\u2500 post \u2502 \u2514\u2500 01_schema_c_do_something.sql \u2502 \u2514\u2500 02_schema_b_do_something.sql \u2502 \u2514\u2500 03_schema_b_do_something.sql \u251c\u2500 apex \u251c\u2500 db \u2502 \u251c\u2500 .hooks \u2502 \u2502 \u2514\u2500 pre \u2502 \u2502 \u2514\u2500 post \u2502 \u251c\u2500 schema_a \u2502 \u2514\u2500 schema_b \u2502 \u2514\u2500 .hooks \u2502 \u2502 \u2514\u2500 pre \u2502 \u2502 \u2514\u2500 post \u2502 \u2514\u2500 ... \u251c\u2500 rest \u251c\u2500 static To execute a hook on a specific object type, you can add this structure to the corresponding directory within the schema hook. \u251c\u2500 db \u2502 \u2514\u2500 schema_a \u2502 \u2514\u2500 .hooks \u2502 \u2502 \u2514\u2500 pre \u2502 \u2502 \u2514\u2500 post \u2502 \u2502 \u2514\u2500 sources \u2502 \u2502 \u2514\u2500 packages \u2502 \u2502 \u2514\u2500 call_me_after_packages_applied.sql \u2502 \u2514\u2500 sources \u2502 \u2514 packages \u2502 \u2514\u2500 my_package.pks \u2502 \u2514\u2500 my_package.pkb","title":"Hooks"},{"location":"concept/#depot","text":"When you create a deployment, whether INIT or PATCH, the deployment artifact is stored in a so-called depot. From this depot, a CI/CD tool, for example, can later fetch the artifact and install it on the target instance. Within the depot directory, the individual deployments are stored in subdirectories that correspond to the Git branch from which they were created. A deployment is stored in the ready subfolder. After a successful installation, the patch, including all logs and temporary files, is placed in the success folder. Errors during the deployment lead to an abort and are stored in the failures subfolder. I recommend to use a separate repository or directory for each stage DB. This has the advantage that the corresponding directories serve their purpose even without Git and possibly access to the development repository. Theoretically, the repository can also be \"doubled\" to have a target directory at home and a source directory at the customer.","title":"Depot"},{"location":"getting_started/","text":"Getting startet Requirements To use dbFlow, all you need is: Git SQLPlus inkl. Oracle Client or SQLcl bash on Windows included as git-bash on MacOS included as bash (old) or zsh (unstable) on Linux it just should work Installation Basically, installing dbFLow consists of nothing more than initializing a directory with git and then cloning the repository as a submodule from GitHub. If you are using an existing git directory / project, cloning the submodule is all you need to do. The cloning of dbFlow as a submodule has the advantage that you can source new features and bugfixes directly from Github. In addition, you always have the possibility to fall back on an older dbFLow version through branching. Warning dbFlow MUST exist as a folder named \".dbFlow\" Starting a new Project # create a folder for your project and change directory into $ mkdir demo && cd demo # init your project with git $ git init # clone dbFlow as submodule $ git submodule add https://github.com/MaikMichel/dbFlow.git .dbFlow Initialize an existing Project # change into your project directory $ cd demo # clone dbFlow as submodule $ git submodule add https://github.com/MaikMichel/dbFlow.git .dbFlow Sometimes it can happen that the bash files cannot be executed. If this is the case, explicit permissions must be granted here. ( chmod +x .dbFlow/*.sh ) Setting up a project To configure a dbFlow project, set it up initially with the command setup.sh generate <project_name> . After that some informations about the project are expected. All entries are stored in the two files build.env and apply.env . build.env contains information about the project itself and apply.env contains information about the database connection and environment. The filename apply.env is stored in the file .gitignore and should not be versioned. Since this file contains environment related information, it is also exactly the one that changes per instance / environment. All settings in this file can be entered also from \"outside\", in order to fill these for example from a CI/CD Tool. Info You can always execute a main shell script/command inside .dbFLow folder to show usage help. Generate Project $ setup.sh generate <project_name> Question Notes Would you like to have a single, multi or flex scheme app (S/M/F) [M] This is the first question. Here you define the project mode. Default is M ulti When running release tests, what is your prefered branch name [build] Later you have the possibility to run so called release tests (NightlyBuilds) . Here you determine the branch name for the tests. The default here is build . Would you like to process changelogs during deployment [Y] dbFlow offers the possibility to generate changlogs based on the commit messages. Here you activate this function. changelog What is the schema name the changelog are processed with [schema_name] WIf you want changelogs to be displayed within your application, you can specify the target schema here, with which the corresponding TemplateCode should be executed. changelog Enter database connections [localhost:1521/xepdb1] Place your connection string like: host:port/service Enter username of admin user (admin, sys, ...) [sys] This user is responsible for all scripts inside db/_setup folder Enter password for sys [leave blank and you will be asked for] Nothing more to clarify. Password is written to apply.env which is mentioned in .gitignore Enter password for deployment_user (proxyuser: flex_depl) [leave blank and you will be asked for] Nothing more to clarify. Password is written to apply.env which is mentioned in .gitignore Enter path to depot [_depot] This is a relative path which points to the depot (artifactory) and is also mentioned in .gitignore when it is not starting with \"..\" Enter stage of this configuration mapped to branch (develop, test, master) [develop] When importing the deployment, this setting assigns the database connection to the source branch Do you wish to generate and install default tooling? (Logger, utPLSQL, teplsql, tapi) [Y] Here you activate the initial installation of the different components/dependencies. These are placed in the db/_setup/features folder. There you can also place other features by yourself. If you don't need certain components, you can delete the corresponding file from the features folder (before running the actual installation).. Install with sql(cl) or sqlplus? [sqlplus] Here you define which CLI dbFLow should use to execute the SQL scripts. Enter application IDs (comma separated) you wish to use initialy (100,101,...) Here you can already enter the application IDs that dbFlow should initially take care of Enter restful Moduls (comma separated) you wish to use initialy (api,test,...) Here you can already specify the REST modules that dbFlow should initially take care of","title":"Getting started"},{"location":"getting_started/#getting-startet","text":"","title":"Getting startet"},{"location":"getting_started/#requirements","text":"To use dbFlow, all you need is: Git SQLPlus inkl. Oracle Client or SQLcl bash on Windows included as git-bash on MacOS included as bash (old) or zsh (unstable) on Linux it just should work","title":"Requirements"},{"location":"getting_started/#installation","text":"Basically, installing dbFLow consists of nothing more than initializing a directory with git and then cloning the repository as a submodule from GitHub. If you are using an existing git directory / project, cloning the submodule is all you need to do. The cloning of dbFlow as a submodule has the advantage that you can source new features and bugfixes directly from Github. In addition, you always have the possibility to fall back on an older dbFLow version through branching. Warning dbFlow MUST exist as a folder named \".dbFlow\"","title":"Installation"},{"location":"getting_started/#starting-a-new-project","text":"# create a folder for your project and change directory into $ mkdir demo && cd demo # init your project with git $ git init # clone dbFlow as submodule $ git submodule add https://github.com/MaikMichel/dbFlow.git .dbFlow","title":"Starting a new Project"},{"location":"getting_started/#initialize-an-existing-project","text":"# change into your project directory $ cd demo # clone dbFlow as submodule $ git submodule add https://github.com/MaikMichel/dbFlow.git .dbFlow Sometimes it can happen that the bash files cannot be executed. If this is the case, explicit permissions must be granted here. ( chmod +x .dbFlow/*.sh )","title":"Initialize an existing Project"},{"location":"getting_started/#setting-up-a-project","text":"To configure a dbFlow project, set it up initially with the command setup.sh generate <project_name> . After that some informations about the project are expected. All entries are stored in the two files build.env and apply.env . build.env contains information about the project itself and apply.env contains information about the database connection and environment. The filename apply.env is stored in the file .gitignore and should not be versioned. Since this file contains environment related information, it is also exactly the one that changes per instance / environment. All settings in this file can be entered also from \"outside\", in order to fill these for example from a CI/CD Tool. Info You can always execute a main shell script/command inside .dbFLow folder to show usage help.","title":"Setting up a project"},{"location":"getting_started/#generate-project","text":"$ setup.sh generate <project_name> Question Notes Would you like to have a single, multi or flex scheme app (S/M/F) [M] This is the first question. Here you define the project mode. Default is M ulti When running release tests, what is your prefered branch name [build] Later you have the possibility to run so called release tests (NightlyBuilds) . Here you determine the branch name for the tests. The default here is build . Would you like to process changelogs during deployment [Y] dbFlow offers the possibility to generate changlogs based on the commit messages. Here you activate this function. changelog What is the schema name the changelog are processed with [schema_name] WIf you want changelogs to be displayed within your application, you can specify the target schema here, with which the corresponding TemplateCode should be executed. changelog Enter database connections [localhost:1521/xepdb1] Place your connection string like: host:port/service Enter username of admin user (admin, sys, ...) [sys] This user is responsible for all scripts inside db/_setup folder Enter password for sys [leave blank and you will be asked for] Nothing more to clarify. Password is written to apply.env which is mentioned in .gitignore Enter password for deployment_user (proxyuser: flex_depl) [leave blank and you will be asked for] Nothing more to clarify. Password is written to apply.env which is mentioned in .gitignore Enter path to depot [_depot] This is a relative path which points to the depot (artifactory) and is also mentioned in .gitignore when it is not starting with \"..\" Enter stage of this configuration mapped to branch (develop, test, master) [develop] When importing the deployment, this setting assigns the database connection to the source branch Do you wish to generate and install default tooling? (Logger, utPLSQL, teplsql, tapi) [Y] Here you activate the initial installation of the different components/dependencies. These are placed in the db/_setup/features folder. There you can also place other features by yourself. If you don't need certain components, you can delete the corresponding file from the features folder (before running the actual installation).. Install with sql(cl) or sqlplus? [sqlplus] Here you define which CLI dbFLow should use to execute the SQL scripts. Enter application IDs (comma separated) you wish to use initialy (100,101,...) Here you can already enter the application IDs that dbFlow should initially take care of Enter restful Moduls (comma separated) you wish to use initialy (api,test,...) Here you can already specify the REST modules that dbFlow should initially take care of","title":"Generate Project"},{"location":"migrating/","text":"Migrating Basic adjustments If you want to migrate from an old version to a new version, you have to do the following steps pullthe current dbFlow version from github Adjust the directory structure from top to bottom Important This means that you have to make the changes in the master branch and merge them through the stage branches to the development branch. This way, there is no new delta to be imported for these adjustments in the next release. # after the changes $...@master: git commit -m \"migrating dbFlow new structure\" $...@master: git push # checkout user acceptence stage $...@master: git checkout uac $...@uac: git merge master $...@uac: git push # checkout user test stage $...@uac: git checkout test $...@test: git merge uac $...@test: git push # checkout user development stage $...@test: git checkout develop $...@develop: git merge test $...@develop: git push Following changes in directory structure from 0.9.8 - stable to 1.0.0 The folder tables_ddl becomes a subfolder of the folder tables . The directories dml/pre and dml/post , and ddl/pre and ddl/post become: dml/patch/pre and dml/patch/post , and ddl/patch/pre and ddl/patch/post . In old versions there was the folder source . This is treated as plural now and must be renamed to sources . Following changes in build.env from 0.9.8 - stable to 1.0.0 have to be done You have to store project mode inside build.env # Following values are valid: SINGLE, MULTI or FLEX PROJECT_MODE = MULTI When you want to use release.sh for nightlybuild, you have to name the build branch # Name of the branch, where release tests are build BUILD_BRANCH = build When you are using the generate changelog feature you have to adjust the following vars # Generate a changelog with these settings # When template.sql file found in reports/changelog then it will be # executed on apply with the CHANGELOG_SCHEMA . # The changelog itself is structured using INTENT_PREFIXES to look # for in commits and to place them in corresponding INTENT_NAMES inside # the file itself. You can define a regexp in TICKET_MATCH to look for # keys to link directly to your ticketsystem using TICKET_URL CHANGELOG_SCHEMA = schema_name INTENT_PREFIXES =( Feat Fix ) INTENT_NAMES =( Features Fixes ) INTENT_ELSE = \"Others\" TICKET_MATCH = \"[A-Z]\\+-[0-9]\\+\" TICKET_URL = \"https://url-to-your-issue-tracker-like-jira/browse\"","title":"Migrating"},{"location":"migrating/#migrating","text":"","title":"Migrating"},{"location":"migrating/#basic-adjustments","text":"If you want to migrate from an old version to a new version, you have to do the following steps pullthe current dbFlow version from github Adjust the directory structure from top to bottom Important This means that you have to make the changes in the master branch and merge them through the stage branches to the development branch. This way, there is no new delta to be imported for these adjustments in the next release. # after the changes $...@master: git commit -m \"migrating dbFlow new structure\" $...@master: git push # checkout user acceptence stage $...@master: git checkout uac $...@uac: git merge master $...@uac: git push # checkout user test stage $...@uac: git checkout test $...@test: git merge uac $...@test: git push # checkout user development stage $...@test: git checkout develop $...@develop: git merge test $...@develop: git push","title":"Basic adjustments"},{"location":"migrating/#following-changes-in-directory-structure-from-098-stable-to-100","text":"The folder tables_ddl becomes a subfolder of the folder tables . The directories dml/pre and dml/post , and ddl/pre and ddl/post become: dml/patch/pre and dml/patch/post , and ddl/patch/pre and ddl/patch/post . In old versions there was the folder source . This is treated as plural now and must be renamed to sources .","title":"Following changes in directory structure from 0.9.8 - stable to 1.0.0"},{"location":"migrating/#following-changes-in-buildenv-from-098-stable-to-100-have-to-be-done","text":"You have to store project mode inside build.env # Following values are valid: SINGLE, MULTI or FLEX PROJECT_MODE = MULTI When you want to use release.sh for nightlybuild, you have to name the build branch # Name of the branch, where release tests are build BUILD_BRANCH = build When you are using the generate changelog feature you have to adjust the following vars # Generate a changelog with these settings # When template.sql file found in reports/changelog then it will be # executed on apply with the CHANGELOG_SCHEMA . # The changelog itself is structured using INTENT_PREFIXES to look # for in commits and to place them in corresponding INTENT_NAMES inside # the file itself. You can define a regexp in TICKET_MATCH to look for # keys to link directly to your ticketsystem using TICKET_URL CHANGELOG_SCHEMA = schema_name INTENT_PREFIXES =( Feat Fix ) INTENT_NAMES =( Features Fixes ) INTENT_ELSE = \"Others\" TICKET_MATCH = \"[A-Z]\\+-[0-9]\\+\" TICKET_URL = \"https://url-to-your-issue-tracker-like-jira/browse\"","title":"Following changes in build.env from 0.9.8 - stable to 1.0.0 have to be done"},{"location":"release/","text":"Release build apply release","title":"Release"},{"location":"release/#release","text":"","title":"Release"},{"location":"release/#build","text":"","title":"build"},{"location":"release/#apply","text":"","title":"apply"},{"location":"release/#release_1","text":"","title":"release"}]}