{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dbFLow The Deployment framework for Oracle Database and APEX Applications","title":"Home"},{"location":"changelog/","text":"Generating Changelogs Intro With dbFlow you have the possibility to create changelogs automatically. These are based on the commit messages of the commits that lead to a patch. The changelog file itself is kept as markdown. Additionally the changelog can be automatically processed (uploaded to the DB) during the deployment if there is a subdirectory named changelog in the reports-folder. Configuration After creating the directory structure and environment files, the following variables were created in the build.env file. CHANGELOG_SCHEMA=<schema_name> INTENT_PREFIXES=( Feat Fix ) INTENT_NAMES=( Features Fixes ) INTENT_ELSE=\"Others\" TICKET_MATCH=\"[A-Z]\\+-[0-9]\\+\" TICKET_URL=\"https://[your-jira-url]/browse\" Name Meaning CHANGELOG_SCHEMA With this scheme the changelog is processed by the reports directory INTENT_PREFIXES The commit messages are grouped with this array of prefixes INTENT_NAMES This array holds the labels of the prefixes, which then appear as headings in the actual changelog INTENT_ELSE All messages not handled by the prefixes appear here TICKET_MATCH RegularExpression to get the ticket number from the commit message TICKET_URL URL to which the ticket number is appended A template.sql file is stored in the reports/changelog directory by default. This file contains the code you have to change to upload this changelog as a clob to a target table. For example like this: /*********************************************** ------- TEMPLATE START ------- l_bin >>> Content as blob of file l_file_name >>> name of the file changelog_init_1.2.3.md ************************************************/ Declare l_version varchar2 ( 100 ); Begin l_version : = substr ( l_file_name , instr ( l_file_name , '_' , 1 , 2 ) + 1 ); l_version : = substr ( l_version , 1 , instr ( l_version , '.' , - 1 , 1 ) - 1 ); -- START custom code begin insert into changelogs ( chl_version , chl_date , chl_content ) values ( l_version , current_date , l_bin ); exception when dup_val_on_index then update changelogs set chl_content = l_bin , chl_date = current_date where chl_version = l_version ; end ; -- END custom code dbms_output . put_line ( gc_green || ' ... Version info uploaded: ' || l_version || gc_reset ); End ; /*********************************************** ------- TEMPLATE END ------- ************************************************/ Example: Project XYZ - Changelog Version 1.2.3 (2022-04-08) Features Changelogs and Version infos are now displayed Emailfrom is included in ref_codes based on stage #XYZ-69 View Managing total revenues per corporations #XYZ-71 View Reporting Dashboard includes total revenues #XYZ-72 View Others Add some unit tests Application processes outsourced to package Color Settings for VSCode Fix Refresh Facetted Search after transaction occured Refactored revenue_totals to be more testable Set missing max length Set plsql searchPath XYZ-50: Changed Buttontext on confirm Email View XYZ-50: Update Email text of invitation View XYZ-74: Aliases in own table View XYZ-74: Nach Nachfrage aus bestehenden Supplier einen Alias machen View","title":"Generating a changelog"},{"location":"changelog/#generating-changelogs","text":"","title":"Generating Changelogs"},{"location":"changelog/#intro","text":"With dbFlow you have the possibility to create changelogs automatically. These are based on the commit messages of the commits that lead to a patch. The changelog file itself is kept as markdown. Additionally the changelog can be automatically processed (uploaded to the DB) during the deployment if there is a subdirectory named changelog in the reports-folder.","title":"Intro"},{"location":"changelog/#configuration","text":"After creating the directory structure and environment files, the following variables were created in the build.env file. CHANGELOG_SCHEMA=<schema_name> INTENT_PREFIXES=( Feat Fix ) INTENT_NAMES=( Features Fixes ) INTENT_ELSE=\"Others\" TICKET_MATCH=\"[A-Z]\\+-[0-9]\\+\" TICKET_URL=\"https://[your-jira-url]/browse\" Name Meaning CHANGELOG_SCHEMA With this scheme the changelog is processed by the reports directory INTENT_PREFIXES The commit messages are grouped with this array of prefixes INTENT_NAMES This array holds the labels of the prefixes, which then appear as headings in the actual changelog INTENT_ELSE All messages not handled by the prefixes appear here TICKET_MATCH RegularExpression to get the ticket number from the commit message TICKET_URL URL to which the ticket number is appended A template.sql file is stored in the reports/changelog directory by default. This file contains the code you have to change to upload this changelog as a clob to a target table. For example like this: /*********************************************** ------- TEMPLATE START ------- l_bin >>> Content as blob of file l_file_name >>> name of the file changelog_init_1.2.3.md ************************************************/ Declare l_version varchar2 ( 100 ); Begin l_version : = substr ( l_file_name , instr ( l_file_name , '_' , 1 , 2 ) + 1 ); l_version : = substr ( l_version , 1 , instr ( l_version , '.' , - 1 , 1 ) - 1 ); -- START custom code begin insert into changelogs ( chl_version , chl_date , chl_content ) values ( l_version , current_date , l_bin ); exception when dup_val_on_index then update changelogs set chl_content = l_bin , chl_date = current_date where chl_version = l_version ; end ; -- END custom code dbms_output . put_line ( gc_green || ' ... Version info uploaded: ' || l_version || gc_reset ); End ; /*********************************************** ------- TEMPLATE END ------- ************************************************/","title":"Configuration"},{"location":"changelog/#example","text":"","title":"Example:"},{"location":"changelog/#project-xyz-changelog","text":"","title":"Project XYZ - Changelog"},{"location":"changelog/#version-123-2022-04-08","text":"","title":"Version 1.2.3 (2022-04-08)"},{"location":"changelog/#features","text":"Changelogs and Version infos are now displayed Emailfrom is included in ref_codes based on stage #XYZ-69 View Managing total revenues per corporations #XYZ-71 View Reporting Dashboard includes total revenues #XYZ-72 View","title":"Features"},{"location":"changelog/#others","text":"Add some unit tests Application processes outsourced to package Color Settings for VSCode Fix Refresh Facetted Search after transaction occured Refactored revenue_totals to be more testable Set missing max length Set plsql searchPath XYZ-50: Changed Buttontext on confirm Email View XYZ-50: Update Email text of invitation View XYZ-74: Aliases in own table View XYZ-74: Nach Nachfrage aus bestehenden Supplier einen Alias machen View","title":"Others"},{"location":"concept/","text":"Concept Git Flow dbFlow is a deployment framework to deploy Oracle based projects / applications. dbFlow is roughly based on the so called Git Flow. This flow describes how releases of an application are delivered from the main development branch to the master branch, production. The following figure outlines this flow. Development mainly takes place on the develop branch. Features that are not included in the next release are developed in so-called feature branches. If a release has to be built from the current development state, the develop branch is merged into the release branch. This represents at the same time a so-called distribution boundary. This means that development can continue on the development branch and does not have to wait for the actual release. The release is first merged into the next higher branch. In our case this is the test branch. If an error is found here, it is fixed in the release branch and merged back to test. These changes also flow back into the development branch later. If the tests on the test stage are successful, the next branch is merged and the release is tested accordingly. This continues until the respective release has reached the master branch and therefore has reached production. dbFlow dbFlow supports the developers and release managers by building an artifact corresponding to the delta of a merge, meaning the changed files, which can be applied to the respective database instance. For each target branch a separate database or database container is expected. The artifacts that dbFLow generates by merging the branches can then be imported into the databases/containers. Modes dbFlow can generate two types of artifacts / supports two types of installation modes, which can be applied to the databases/containers. On the one hand this is a so-called INIT ial delivery, which empties the target schemas first, or expects empty schemas, and on the other hand a PATCH delivery, which requires the corresponding previous version of a deployment on the database. Info Usually, the initial delivery is only installed ONCE on a production system. After that, only patch versions are applied. The advantage of this two-track approach is that a version of an application can always be built directly and thus forms the basis of a subsequent patch. For example, one can build the initial version of a predecessor and install the current state as a patch on top of it. This procedure forms the basis of flawless releases. With this approach it is easy to map the concept of NighlyBuilds with Jenkins or similar CI/CD tools. To test whether a release can be applied to Prod/Master, it is sufficient to create an initial release of Master and apply it to a test database. Afterwards, the patch, in other words the delta to the development branch, is applied. Important In INIT mode, all files from all directories except those named with Patch are applied. In PATCH mode only the changed files are applied. Files inside .hook - folders will be always executed Main Folders To support this concept a dbFlow project and its directory structure must have a certain structure. dbFLow expects the following directories at the root level of the project itself. Folder Description apex All APEX application will be stored in this folder db This folder contains the required database schemas and their objects. db/_setup This folder contains the required objects your application depends on. All scripts inside are called by the preferred admin account you configured (sys, admin, ...) rest Here the REST services / modules are stored reports binary files for templating purpose static In this folder the files are stored, which you upload later with the upload command to the static files of the respective application. (managed by dbFlux ) Project Types This structure is independent from the project-type. dbFlow knows 3 types of projects: SingleSchema, MultiSchema and FlexSchema SingleSchema and MultiSchema are very similar. APEX applications and REST modules are imported or exported with the application user. The name of the workspace is stored in both modes in the project configuration. In MultiSchema mode two additional schemas (LOGIC and DATA) are used. Here a three layer model is built up, which holds the data in the DATA schema, the business logic in the LOGIC schema and everything that is application specific in the APP schema. In SingleSchema mode, a direct database connection is established with the respective schema. In MultiSchema mode, as well as in FlexSchema mode, a ProxyUser is used for the database connection, which then connects to the target schema. If the project is configured in FlexMode , you can freely configure the corresponding schemas and workspaces via the directory structure. When using FlexMode , APEX applications are stored within workspace folders, which are again located within schema folders. The same applies to the static subdirectories, of course. REST modules are also stored in their own schema folder in FlexMode . Importing or exporting is done via the proxy user, which connects to the respective schema, which is configured by directory name. Schema Folders Each schemafolder inside db folder except _setup is build with the same structur. Folder Description db .. schema .... constraints Constraints are stored here and subdivided according to their type ...... checks Check Constraints ...... foreigns Foreign Keys ...... primaries Primary Keys ...... uniques Unique Keys .... contexts Sys_Contexts when needed .... ddl DDL Scripts for deployment subddivided on deployment mode ...... init Init scripts are executed only for the first installation (init). ...... patch Patch scripts are executed only in the update case (patch) and are divided into scripts that are executed at the beginning or at the end of the respective update. ........ post ........ pre .... dml DML Scripts for deployment subddivided on deployment mode ...... base Base scripts are always executed, no matter in what mode the deployment is used (first install - init) or (update - patch). Therefore they must be restartable. ...... init Init scripts are executed only for the first installation (init). ...... patch Patch scripts are executed only in the update case (patch) and are divided into scripts that are executed at the beginning or at the end of the respective update. ........ post ........ pre .... indexes Indexes are stored here and subdivided according to their type ...... defaults Non uniqe indexes ...... primaries Unique Indexes based in primary key columns ...... uniques Unique Indexes .... jobs Jobs, Scheduler scripts goes here .... policies Policies .... sequences Sequences must be scripted in a restartable manner .... sources All PL/SQL Code is stored in respective subfolders ...... functions ...... packages Extension for package specification is pks and extension pkb is used for body ...... procedures ...... triggers ...... types .... views Views goes here .... tables Here are all create table scripts stored ...... tables_ddl All table alter or modification scripts named with tablename.num.sql goes here .....tests Unittests ...... packages Packages containing utPLSQL Unittests All files must be stored as executable SQL scripts. dbFlow uses SQLPlus or SQLcl to import these scripts. Warning Don't forget the trailing slashes in PLSQL files The file structure has several purposes. On the one hand, these directories represent a fixed point in time during delivery, i.e. they reflect the order of execution. For example, tables are applied before the indexes and the constraints. Primary key constraints are applied before foreign key constraints, and so on. In addition, the division of the table directory into tables and tables_ddl represents a kind of toggle. In case of an initial delivery only the scripts from the table directory are executed. If it is a patch delivery, the script from the table directory will not be executed if files with the same root name exist in the tables_ddl directory. For example, if the table employees changes, the file is updated accordingly with the actual create-table statement. In addition, a matching alter-table script with the same base name is stored in the tables_ddl directory. // employees . sql create table employees ( emp_id number not null , emp_name varchar2 ( 250 char ) not null , emp_birthday date -- new column ); // employees . 1 . sql alter table employees add ( emp_birthday date -- new column ); files content tables - table_ddl - - employees.1.sql alter table add (..); - employees.sql create table (..); Tip Changes to tables are always made 2 times. Once for a new installation (init) and once for an update (patch). Table scripts may only contain the pure tables and comments. Constraints and indexes must be stored in the appropriate directories. Files from the table directory are executed if it is a new installation (init) or if the table is to be newly created in this update (patch). APEX The used applications are stored in the apex directory. The applications will be expected in splitted form. For each application there is a corresponding folder containing the respective files (standard APEX export). For a deployment only the appropriate changes are included. A configuration option can be used here, in order to include all files into the deployment. \u251c\u2500 apex \u2502 \u2514\u2500 f1000 \u2502 \u2502 \u2514\u2500 application \u2502 \u2502 \u2514\u2500 install.sql \u2502 \u2514\u2500 f2000 In SingleSchema and MultiSchema mode, the workspace is stored in the project configuration. The target schema is always the APP schema. If you use FlexSchema mode, however, this information is stored in the directory tree itself. \u251c\u2500 apex \u2502 \u2514\u2500 schema_a \u2502 \u2502 \u2514\u2500 workspace_x \u2502 \u2502 \u2514\u2500 f1000 \u2502 \u2514\u2500 schema_b \u2502 \u2514\u2500 workspace_y \u2502 \u2514\u2500 f2000 REST If you use REST modules in your project, they are placed in the rest folder. Here dbFlow also expects a certain directory structure. This has a direct influence on the order of execution during the deployment. \u251c\u2500 rest \u2502 \u2514\u2500 access \u2502 \u2502 \u2514\u2500 roles \u2502 \u2502 \u2514\u2500 privileges \u2502 \u2502 \u2514\u2500 mapping \u2502 \u2514\u2500 module \u2502 \u2514\u2500 module_name_a \u2502 \u2502 \u2514\u2500 module_name_a.module.sql \u2502 \u2514\u2500 module_name_b \u2502 \u2514\u2500 module_name_b.module.sql \u2502 \u2514\u2500 module_name_b.condition.sql Files are imported into these folders in the following order. access/roles access/privileges access/mapping modules Within the folders the order is alphabetical. If dbFLow finds a module file with the same file base and the extension *.condition.sql, this condition will be included in the install script. Important In the Condtion file a PLSQL expression is expected which returns a boolean value. Later this expression becomes part of an IF condition. Hooks On each level of the directories there are so called .hooks folders. You can find them in the root directory, in the db folder and in the schema folders. The .hooks folders are always divided into the subfolders pre and post . During the deployment process the scripts there will be executed in alphabetically order. These type of folders are meant to hold scripts which won't change too much during lifecycle of your product. For example you could place some kind of generator script inside here (Gen TableAPI, ...). Note Hookscripts outside the respective schema folders must contain the corresponding target schema in the name. \u251c\u2500 .hooks \u2502 \u251c\u2500 pre \u2502 \u2502 \u2514\u2500 01_schema_a_do_something.sql \u2502 \u2502 \u2514\u2500 02_schema_a_do_something.sql \u2502 \u2514\u2500 post \u2502 \u2514\u2500 01_schema_c_do_something.sql \u2502 \u2514\u2500 02_schema_b_do_something.sql \u2502 \u2514\u2500 03_schema_b_do_something.sql \u251c\u2500 apex \u251c\u2500 db \u2502 \u251c\u2500 .hooks \u2502 \u2502 \u2514\u2500 pre \u2502 \u2502 \u2514\u2500 post \u2502 \u251c\u2500 schema_a \u2502 \u2514\u2500 schema_b \u2502 \u2514\u2500 .hooks \u2502 \u2502 \u2514\u2500 pre \u2502 \u2502 \u2514\u2500 post \u2502 \u2514\u2500 ... \u251c\u2500 rest \u251c\u2500 static To execute a hook on a specific object type, you can add this structure to the corresponding directory within the schema hook. \u251c\u2500 db \u2502 \u2514\u2500 schema_a \u2502 \u2514\u2500 .hooks \u2502 \u2502 \u2514\u2500 pre \u2502 \u2502 \u2514\u2500 post \u2502 \u2502 \u2514\u2500 sources \u2502 \u2502 \u2514\u2500 packages \u2502 \u2502 \u2514\u2500 call_me_after_packages_applied.sql \u2502 \u2514\u2500 sources \u2502 \u2514 packages \u2502 \u2514\u2500 my_package.pks \u2502 \u2514\u2500 my_package.pkb Depot When you create a deployment, whether INIT or PATCH, the deployment artifact is stored in a so-called depot. From this depot, a CI/CD tool, for example, can later fetch the artifact and install it on the target instance. Within the depot directory, the individual deployments are stored in subdirectories that correspond to the Git branch from which they were created. A deployment is stored in the ready subfolder. After a successful installation, the patch, including all logs and temporary files, is placed in the success folder. Errors during the deployment lead to an abort and are stored in the failures subfolder. I recommend to use a separate repository or directory for each stage DB. This has the advantage that the corresponding directories serve their purpose even without Git and possibly access to the development repository. Theoretically, the repository can also be \"doubled\" to have a target directory at home and a source directory at the customer.","title":"The Concept"},{"location":"concept/#concept","text":"","title":"Concept"},{"location":"concept/#git-flow","text":"dbFlow is a deployment framework to deploy Oracle based projects / applications. dbFlow is roughly based on the so called Git Flow. This flow describes how releases of an application are delivered from the main development branch to the master branch, production. The following figure outlines this flow. Development mainly takes place on the develop branch. Features that are not included in the next release are developed in so-called feature branches. If a release has to be built from the current development state, the develop branch is merged into the release branch. This represents at the same time a so-called distribution boundary. This means that development can continue on the development branch and does not have to wait for the actual release. The release is first merged into the next higher branch. In our case this is the test branch. If an error is found here, it is fixed in the release branch and merged back to test. These changes also flow back into the development branch later. If the tests on the test stage are successful, the next branch is merged and the release is tested accordingly. This continues until the respective release has reached the master branch and therefore has reached production.","title":"Git Flow"},{"location":"concept/#dbflow","text":"dbFlow supports the developers and release managers by building an artifact corresponding to the delta of a merge, meaning the changed files, which can be applied to the respective database instance. For each target branch a separate database or database container is expected. The artifacts that dbFLow generates by merging the branches can then be imported into the databases/containers.","title":"dbFlow"},{"location":"concept/#modes","text":"dbFlow can generate two types of artifacts / supports two types of installation modes, which can be applied to the databases/containers. On the one hand this is a so-called INIT ial delivery, which empties the target schemas first, or expects empty schemas, and on the other hand a PATCH delivery, which requires the corresponding previous version of a deployment on the database. Info Usually, the initial delivery is only installed ONCE on a production system. After that, only patch versions are applied. The advantage of this two-track approach is that a version of an application can always be built directly and thus forms the basis of a subsequent patch. For example, one can build the initial version of a predecessor and install the current state as a patch on top of it. This procedure forms the basis of flawless releases. With this approach it is easy to map the concept of NighlyBuilds with Jenkins or similar CI/CD tools. To test whether a release can be applied to Prod/Master, it is sufficient to create an initial release of Master and apply it to a test database. Afterwards, the patch, in other words the delta to the development branch, is applied. Important In INIT mode, all files from all directories except those named with Patch are applied. In PATCH mode only the changed files are applied. Files inside .hook - folders will be always executed","title":"Modes"},{"location":"concept/#main-folders","text":"To support this concept a dbFlow project and its directory structure must have a certain structure. dbFLow expects the following directories at the root level of the project itself. Folder Description apex All APEX application will be stored in this folder db This folder contains the required database schemas and their objects. db/_setup This folder contains the required objects your application depends on. All scripts inside are called by the preferred admin account you configured (sys, admin, ...) rest Here the REST services / modules are stored reports binary files for templating purpose static In this folder the files are stored, which you upload later with the upload command to the static files of the respective application. (managed by dbFlux )","title":"Main Folders"},{"location":"concept/#project-types","text":"This structure is independent from the project-type. dbFlow knows 3 types of projects: SingleSchema, MultiSchema and FlexSchema SingleSchema and MultiSchema are very similar. APEX applications and REST modules are imported or exported with the application user. The name of the workspace is stored in both modes in the project configuration. In MultiSchema mode two additional schemas (LOGIC and DATA) are used. Here a three layer model is built up, which holds the data in the DATA schema, the business logic in the LOGIC schema and everything that is application specific in the APP schema. In SingleSchema mode, a direct database connection is established with the respective schema. In MultiSchema mode, as well as in FlexSchema mode, a ProxyUser is used for the database connection, which then connects to the target schema. If the project is configured in FlexMode , you can freely configure the corresponding schemas and workspaces via the directory structure. When using FlexMode , APEX applications are stored within workspace folders, which are again located within schema folders. The same applies to the static subdirectories, of course. REST modules are also stored in their own schema folder in FlexMode . Importing or exporting is done via the proxy user, which connects to the respective schema, which is configured by directory name.","title":"Project Types"},{"location":"concept/#schema-folders","text":"Each schemafolder inside db folder except _setup is build with the same structur. Folder Description db .. schema .... constraints Constraints are stored here and subdivided according to their type ...... checks Check Constraints ...... foreigns Foreign Keys ...... primaries Primary Keys ...... uniques Unique Keys .... contexts Sys_Contexts when needed .... ddl DDL Scripts for deployment subddivided on deployment mode ...... init Init scripts are executed only for the first installation (init). ...... patch Patch scripts are executed only in the update case (patch) and are divided into scripts that are executed at the beginning or at the end of the respective update. ........ post ........ pre .... dml DML Scripts for deployment subddivided on deployment mode ...... base Base scripts are always executed, no matter in what mode the deployment is used (first install - init) or (update - patch). Therefore they must be restartable. ...... init Init scripts are executed only for the first installation (init). ...... patch Patch scripts are executed only in the update case (patch) and are divided into scripts that are executed at the beginning or at the end of the respective update. ........ post ........ pre .... indexes Indexes are stored here and subdivided according to their type ...... defaults Non uniqe indexes ...... primaries Unique Indexes based in primary key columns ...... uniques Unique Indexes .... jobs Jobs, Scheduler scripts goes here .... policies Policies .... sequences Sequences must be scripted in a restartable manner .... sources All PL/SQL Code is stored in respective subfolders ...... functions ...... packages Extension for package specification is pks and extension pkb is used for body ...... procedures ...... triggers ...... types .... views Views goes here .... tables Here are all create table scripts stored ...... tables_ddl All table alter or modification scripts named with tablename.num.sql goes here .....tests Unittests ...... packages Packages containing utPLSQL Unittests All files must be stored as executable SQL scripts. dbFlow uses SQLPlus or SQLcl to import these scripts. Warning Don't forget the trailing slashes in PLSQL files The file structure has several purposes. On the one hand, these directories represent a fixed point in time during delivery, i.e. they reflect the order of execution. For example, tables are applied before the indexes and the constraints. Primary key constraints are applied before foreign key constraints, and so on. In addition, the division of the table directory into tables and tables_ddl represents a kind of toggle. In case of an initial delivery only the scripts from the table directory are executed. If it is a patch delivery, the script from the table directory will not be executed if files with the same root name exist in the tables_ddl directory. For example, if the table employees changes, the file is updated accordingly with the actual create-table statement. In addition, a matching alter-table script with the same base name is stored in the tables_ddl directory. // employees . sql create table employees ( emp_id number not null , emp_name varchar2 ( 250 char ) not null , emp_birthday date -- new column ); // employees . 1 . sql alter table employees add ( emp_birthday date -- new column ); files content tables - table_ddl - - employees.1.sql alter table add (..); - employees.sql create table (..); Tip Changes to tables are always made 2 times. Once for a new installation (init) and once for an update (patch). Table scripts may only contain the pure tables and comments. Constraints and indexes must be stored in the appropriate directories. Files from the table directory are executed if it is a new installation (init) or if the table is to be newly created in this update (patch).","title":"Schema Folders"},{"location":"concept/#apex","text":"The used applications are stored in the apex directory. The applications will be expected in splitted form. For each application there is a corresponding folder containing the respective files (standard APEX export). For a deployment only the appropriate changes are included. A configuration option can be used here, in order to include all files into the deployment. \u251c\u2500 apex \u2502 \u2514\u2500 f1000 \u2502 \u2502 \u2514\u2500 application \u2502 \u2502 \u2514\u2500 install.sql \u2502 \u2514\u2500 f2000 In SingleSchema and MultiSchema mode, the workspace is stored in the project configuration. The target schema is always the APP schema. If you use FlexSchema mode, however, this information is stored in the directory tree itself. \u251c\u2500 apex \u2502 \u2514\u2500 schema_a \u2502 \u2502 \u2514\u2500 workspace_x \u2502 \u2502 \u2514\u2500 f1000 \u2502 \u2514\u2500 schema_b \u2502 \u2514\u2500 workspace_y \u2502 \u2514\u2500 f2000","title":"APEX"},{"location":"concept/#rest","text":"If you use REST modules in your project, they are placed in the rest folder. Here dbFlow also expects a certain directory structure. This has a direct influence on the order of execution during the deployment. \u251c\u2500 rest \u2502 \u2514\u2500 access \u2502 \u2502 \u2514\u2500 roles \u2502 \u2502 \u2514\u2500 privileges \u2502 \u2502 \u2514\u2500 mapping \u2502 \u2514\u2500 module \u2502 \u2514\u2500 module_name_a \u2502 \u2502 \u2514\u2500 module_name_a.module.sql \u2502 \u2514\u2500 module_name_b \u2502 \u2514\u2500 module_name_b.module.sql \u2502 \u2514\u2500 module_name_b.condition.sql Files are imported into these folders in the following order. access/roles access/privileges access/mapping modules Within the folders the order is alphabetical. If dbFLow finds a module file with the same file base and the extension *.condition.sql, this condition will be included in the install script. Important In the Condtion file a PLSQL expression is expected which returns a boolean value. Later this expression becomes part of an IF condition.","title":"REST"},{"location":"concept/#hooks","text":"On each level of the directories there are so called .hooks folders. You can find them in the root directory, in the db folder and in the schema folders. The .hooks folders are always divided into the subfolders pre and post . During the deployment process the scripts there will be executed in alphabetically order. These type of folders are meant to hold scripts which won't change too much during lifecycle of your product. For example you could place some kind of generator script inside here (Gen TableAPI, ...). Note Hookscripts outside the respective schema folders must contain the corresponding target schema in the name. \u251c\u2500 .hooks \u2502 \u251c\u2500 pre \u2502 \u2502 \u2514\u2500 01_schema_a_do_something.sql \u2502 \u2502 \u2514\u2500 02_schema_a_do_something.sql \u2502 \u2514\u2500 post \u2502 \u2514\u2500 01_schema_c_do_something.sql \u2502 \u2514\u2500 02_schema_b_do_something.sql \u2502 \u2514\u2500 03_schema_b_do_something.sql \u251c\u2500 apex \u251c\u2500 db \u2502 \u251c\u2500 .hooks \u2502 \u2502 \u2514\u2500 pre \u2502 \u2502 \u2514\u2500 post \u2502 \u251c\u2500 schema_a \u2502 \u2514\u2500 schema_b \u2502 \u2514\u2500 .hooks \u2502 \u2502 \u2514\u2500 pre \u2502 \u2502 \u2514\u2500 post \u2502 \u2514\u2500 ... \u251c\u2500 rest \u251c\u2500 static To execute a hook on a specific object type, you can add this structure to the corresponding directory within the schema hook. \u251c\u2500 db \u2502 \u2514\u2500 schema_a \u2502 \u2514\u2500 .hooks \u2502 \u2502 \u2514\u2500 pre \u2502 \u2502 \u2514\u2500 post \u2502 \u2502 \u2514\u2500 sources \u2502 \u2502 \u2514\u2500 packages \u2502 \u2502 \u2514\u2500 call_me_after_packages_applied.sql \u2502 \u2514\u2500 sources \u2502 \u2514 packages \u2502 \u2514\u2500 my_package.pks \u2502 \u2514\u2500 my_package.pkb","title":"Hooks"},{"location":"concept/#depot","text":"When you create a deployment, whether INIT or PATCH, the deployment artifact is stored in a so-called depot. From this depot, a CI/CD tool, for example, can later fetch the artifact and install it on the target instance. Within the depot directory, the individual deployments are stored in subdirectories that correspond to the Git branch from which they were created. A deployment is stored in the ready subfolder. After a successful installation, the patch, including all logs and temporary files, is placed in the success folder. Errors during the deployment lead to an abort and are stored in the failures subfolder. I recommend to use a separate repository or directory for each stage DB. This has the advantage that the corresponding directories serve their purpose even without Git and possibly access to the development repository. Theoretically, the repository can also be \"doubled\" to have a target directory at home and a source directory at the customer.","title":"Depot"},{"location":"deployment/","text":"Deployment As already described in the concept dbFlow knows two types of releases. One is the initial release, which imports the complete product against empty schemas or clears them first, and the other is the patch release, which determines the delta of two commit levels and applies it on top to an existing installation. A release is created in two steps. Step 1: build - This is where the artifact is created. Step 2: apply - This is where the artifact is deployed to the respective target environment. Warning Remember, during an init, the target schemas are cleared at the beginning. All objects will be dropped! build The build.sh script is used to create the so-called build file or artifact. This is used to create the actual release based on the state of the files in the directory tree / Git repo. As with any dbFlow script, you can display the possible parameters by omitting the parameters or explicitly with the --help parameter. The build.sh script creates a tar ball with all relevant files and stores it in the depot. The location of the depot is determined in the file apply.env. init In an init ial release, all files from the database directories are determined and imported in a specific order. The files from the directories \\*/[ddl|dml]/patch/\\* and \\*/tables/tables_ddl are ignored. By using the flag -i/--init an init ial release is created. Additionally you need the target version of the release. With the flag -v/--version you name the version of the release. The release itself is created from the current branch. If you want to create a release from the test branch, you have to switch there with Git first. Order of the directories .hooks/pre sequences tables indexes/primaries indexes/uniques indexes/defaults constraints/primaries constraints/foreigns constraints/checks constraints/uniques contexts policies sources/types sources/packages sources/functions sources/procedures views mviews sources/triggers jobs tests/packages ddl/init dml/init dml/base .hooks/post $ .dbFlow/build.sh --init --version 1 .0.0 If you name the version with \"install\" then the release will be applied directly. patch In a patch release, all changed files are determined from the database directories and applied in a specific order. Which files are considered as modified is determined by Git. With the parameters -s/--start (defaults to ORIG_HEAD) and -e/--end (defaults to HEAD) one can set these parameters explicitly. Which files become part of the patch can be output by using the -l/--listfiles flag. Start should always be at least one commit prior to the end commit. The files from the directories \\*/[ddl|dml]/init/\\* are ignored. Additionally there is the import switch, which says, that if the table to be changed exists in the table folder AND in the table_ddl folder, ONLY the files with the same name from the table_ddl folder are imported. By using the flag -p/--patch a patch release is created. Additionally you need the target version of the release. With the flag -v/--version you name the version of the release. Order of the directories .hooks/pre ddl/patch/pre_${branch} dml/patch/pre_${branch} ddl/patch/pre dml/patch/pre sequences tables tables/tables_ddl indexes/primaries indexes/uniques indexes/defaults constraints/primaries constraints/foreigns constraints/checks constraints/uniques contexts policies sources/types sources/packages sources/functions sources/procedures views mviews sources/triggers jobs tests/packages ddl/patch/post_${branch} dml/patch/post_${branch} ddl/patch/post dml/base dml/patch/post .hooks/post $ .dbFlow/build.sh --patch --version 1 .1.0 $ .dbFlow/build.sh --patch --version 1 .2.0 --start 1 .0.0 $ .dbFlow/build.sh --patch --version 1 .3.0 --start 71563f65 --end ba12010a $ .dbFlow/build.sh --patch --version 1 .4.0 --start ORIG_HEAD --end HEAD For example, by using stage branches, you can merge the current state of the develop branch into the test branch and build the release by implicitly using the Git variables HEAD and ORIG_HEAD. # make shure you have all changes @develop$ git pull # goto branch mapped to test-Stage @develop$ git checkout test # again: make shure you have all changes @test$ git pull # merge all chages from develop @test$ git merge develop # build the relase artifact @test$ .dbFlow/build.sh --patch --version 1 .2.0 release You don't have to do these steps manually if you use the release.sh script. Here you can simply specify your target branch and the script does the appropriate steps to merge the respective branches itself. By omitting the parameters you can also see what is needed. release.sh does the handling / merging of the branches for you and calls build.sh afterwards. .dbFlow/release.sh --target master --version 1 .2.3 To do a release test, the release script can build three artifacts for you (flag -b/--build ). This functionality is for the so-called nightly builds. Here an initial release of the predecessor, a patch of the successor, and an initial release of the successor are built. Using a CI/CD server, such as Jenkins, you can then create these 3 artifacts and install them one after the other on the corresponding instance and of course test them. .dbFlow/release.sh --source develop --target master -b apply The apply.sh command applies a release to the respective configured database. The artifact is fetched from the depot and unpacked into the appropriate directory. Afterwards the installation scripts are executed. The variable STAGE, from the file apply.env, is used to determine the source directory of a release. The build script stores the artifacts in a directory in a depot that contains the current branch name. The naming of the variable STAGE is now used to get the artifact matching the stage / database connection. With this method there can be n instances, which all point to the same depot directory. Info I recommend to create a corresponding instance directory for each database and to version it as well. This directory should also contain a .dbFlow - submodule. It is sufficient to copy the files apply.env and build.env into the directory and to adjust the database connection and the stage. init Warning Importing an init release leads to data loss! By specifying the -i/--init flag, an init release is retrieved from the depot and then imported. If no password is stored in the apply.env (this is recommended), it will be requested at the very beginning. $ .dbFlow/apply.sh --init --version 1 .0.0 patch By specifying the -p/--patch flag, a patch release is searched for in the depot and then applied. If no password is stored in the apply.env (this is recommended), it will be requested at the very beginning. $ .dbFlow/apply.sh --patch --version 1 .1.0 After the installation of a release, the artifact, as well as all resulting log and installation files are stored in the depot under the directory (success or failure). Failure of course only if the installation was aborted due to an error. Now it can happen that you want to continue the installation after an interruption at the same place. For this the parameter -r/--redolog is used. If you specify the log file of the previous installation, it will be analyzed and continued with the step that leads to an abort.","title":"How to deploy"},{"location":"deployment/#deployment","text":"As already described in the concept dbFlow knows two types of releases. One is the initial release, which imports the complete product against empty schemas or clears them first, and the other is the patch release, which determines the delta of two commit levels and applies it on top to an existing installation. A release is created in two steps. Step 1: build - This is where the artifact is created. Step 2: apply - This is where the artifact is deployed to the respective target environment. Warning Remember, during an init, the target schemas are cleared at the beginning. All objects will be dropped!","title":"Deployment"},{"location":"deployment/#build","text":"The build.sh script is used to create the so-called build file or artifact. This is used to create the actual release based on the state of the files in the directory tree / Git repo. As with any dbFlow script, you can display the possible parameters by omitting the parameters or explicitly with the --help parameter. The build.sh script creates a tar ball with all relevant files and stores it in the depot. The location of the depot is determined in the file apply.env.","title":"build"},{"location":"deployment/#init","text":"In an init ial release, all files from the database directories are determined and imported in a specific order. The files from the directories \\*/[ddl|dml]/patch/\\* and \\*/tables/tables_ddl are ignored. By using the flag -i/--init an init ial release is created. Additionally you need the target version of the release. With the flag -v/--version you name the version of the release. The release itself is created from the current branch. If you want to create a release from the test branch, you have to switch there with Git first. Order of the directories .hooks/pre sequences tables indexes/primaries indexes/uniques indexes/defaults constraints/primaries constraints/foreigns constraints/checks constraints/uniques contexts policies sources/types sources/packages sources/functions sources/procedures views mviews sources/triggers jobs tests/packages ddl/init dml/init dml/base .hooks/post $ .dbFlow/build.sh --init --version 1 .0.0 If you name the version with \"install\" then the release will be applied directly.","title":"init"},{"location":"deployment/#patch","text":"In a patch release, all changed files are determined from the database directories and applied in a specific order. Which files are considered as modified is determined by Git. With the parameters -s/--start (defaults to ORIG_HEAD) and -e/--end (defaults to HEAD) one can set these parameters explicitly. Which files become part of the patch can be output by using the -l/--listfiles flag. Start should always be at least one commit prior to the end commit. The files from the directories \\*/[ddl|dml]/init/\\* are ignored. Additionally there is the import switch, which says, that if the table to be changed exists in the table folder AND in the table_ddl folder, ONLY the files with the same name from the table_ddl folder are imported. By using the flag -p/--patch a patch release is created. Additionally you need the target version of the release. With the flag -v/--version you name the version of the release. Order of the directories .hooks/pre ddl/patch/pre_${branch} dml/patch/pre_${branch} ddl/patch/pre dml/patch/pre sequences tables tables/tables_ddl indexes/primaries indexes/uniques indexes/defaults constraints/primaries constraints/foreigns constraints/checks constraints/uniques contexts policies sources/types sources/packages sources/functions sources/procedures views mviews sources/triggers jobs tests/packages ddl/patch/post_${branch} dml/patch/post_${branch} ddl/patch/post dml/base dml/patch/post .hooks/post $ .dbFlow/build.sh --patch --version 1 .1.0 $ .dbFlow/build.sh --patch --version 1 .2.0 --start 1 .0.0 $ .dbFlow/build.sh --patch --version 1 .3.0 --start 71563f65 --end ba12010a $ .dbFlow/build.sh --patch --version 1 .4.0 --start ORIG_HEAD --end HEAD For example, by using stage branches, you can merge the current state of the develop branch into the test branch and build the release by implicitly using the Git variables HEAD and ORIG_HEAD. # make shure you have all changes @develop$ git pull # goto branch mapped to test-Stage @develop$ git checkout test # again: make shure you have all changes @test$ git pull # merge all chages from develop @test$ git merge develop # build the relase artifact @test$ .dbFlow/build.sh --patch --version 1 .2.0","title":"patch"},{"location":"deployment/#release","text":"You don't have to do these steps manually if you use the release.sh script. Here you can simply specify your target branch and the script does the appropriate steps to merge the respective branches itself. By omitting the parameters you can also see what is needed. release.sh does the handling / merging of the branches for you and calls build.sh afterwards. .dbFlow/release.sh --target master --version 1 .2.3 To do a release test, the release script can build three artifacts for you (flag -b/--build ). This functionality is for the so-called nightly builds. Here an initial release of the predecessor, a patch of the successor, and an initial release of the successor are built. Using a CI/CD server, such as Jenkins, you can then create these 3 artifacts and install them one after the other on the corresponding instance and of course test them. .dbFlow/release.sh --source develop --target master -b","title":"release"},{"location":"deployment/#apply","text":"The apply.sh command applies a release to the respective configured database. The artifact is fetched from the depot and unpacked into the appropriate directory. Afterwards the installation scripts are executed. The variable STAGE, from the file apply.env, is used to determine the source directory of a release. The build script stores the artifacts in a directory in a depot that contains the current branch name. The naming of the variable STAGE is now used to get the artifact matching the stage / database connection. With this method there can be n instances, which all point to the same depot directory. Info I recommend to create a corresponding instance directory for each database and to version it as well. This directory should also contain a .dbFlow - submodule. It is sufficient to copy the files apply.env and build.env into the directory and to adjust the database connection and the stage.","title":"apply"},{"location":"deployment/#init_1","text":"Warning Importing an init release leads to data loss! By specifying the -i/--init flag, an init release is retrieved from the depot and then imported. If no password is stored in the apply.env (this is recommended), it will be requested at the very beginning. $ .dbFlow/apply.sh --init --version 1 .0.0","title":"init"},{"location":"deployment/#patch_1","text":"By specifying the -p/--patch flag, a patch release is searched for in the depot and then applied. If no password is stored in the apply.env (this is recommended), it will be requested at the very beginning. $ .dbFlow/apply.sh --patch --version 1 .1.0 After the installation of a release, the artifact, as well as all resulting log and installation files are stored in the depot under the directory (success or failure). Failure of course only if the installation was aborted due to an error. Now it can happen that you want to continue the installation after an interruption at the same place. For this the parameter -r/--redolog is used. If you specify the log file of the previous installation, it will be analyzed and continued with the step that leads to an abort.","title":"patch"},{"location":"getting_started/","text":"Getting startet Requirements To use dbFlow, all you need is: Git SQLPlus inkl. Oracle Client or SQLcl bash on Windows included as git-bash on MacOS included as bash (old) or zsh (unstable) on Linux it just should work Installation Basically, installing dbFLow consists of nothing more than initializing a directory with git and then cloning the repository as a submodule from GitHub. If you are using an existing git directory / project, cloning the submodule is all you need to do. The cloning of dbFlow as a submodule has the advantage that you can source new features and bugfixes directly from Github. In addition, you always have the possibility to fall back on an older dbFLow version through branching. Warning dbFlow MUST exist as a folder named \".dbFlow\" Starting a new Project # create a folder for your project and change directory into $ mkdir demo && cd demo # init your project with git $ git init # clone dbFlow as submodule $ git submodule add https://github.com/MaikMichel/dbFlow.git .dbFlow Initialize an existing Project # change into your project directory $ cd demo # clone dbFlow as submodule $ git submodule add https://github.com/MaikMichel/dbFlow.git .dbFlow Sometimes it can happen that the bash files cannot be executed. If this is the case, explicit permissions must be granted here. ( chmod +x .dbFlow/*.sh ) Setting up a project To configure a dbFlow project, set it up initially with the command setup.sh generate <project_name> . After that some informations about the project are expected. All entries are stored in the two files build.env and apply.env . build.env contains information about the project itself and apply.env contains information about the database connection and environment. The filename apply.env is stored in the file .gitignore and should not be versioned. Since this file contains environment related information, it is also exactly the one that changes per instance / environment. All settings in this file can be entered also from \"outside\", in order to fill these for example from a CI/CD Tool. Info You can always execute a main shell script/command inside .dbFLow folder to show usage help. Generate Project $ .dbFlow/setup.sh generate <project_name> Question Notes Would you like to have a single, multi or flex scheme app (S/M/F) [M] This is the first question. Here you define the project mode. Default is M ulti When running release tests, what is your prefered branch name [build] Later you have the possibility to run so called release tests (NightlyBuilds) . Here you determine the branch name for the tests. The default here is build . Would you like to process changelogs during deployment [Y] dbFlow offers the possibility to generate changlogs based on the commit messages. Here you activate this function. changelog What is the schema name the changelog are processed with [schema_name] WIf you want changelogs to be displayed within your application, you can specify the target schema here, with which the corresponding TemplateCode should be executed. changelog Enter database connections [localhost:1521/xepdb1] Place your connection string like: host:port/service Enter username of admin user (admin, sys, ...) [sys] This user is responsible for all scripts inside db/_setup folder Enter password for sys [leave blank and you will be asked for] Nothing more to clarify. Password is written to apply.env which is mentioned in .gitignore Enter password for deployment_user (proxyuser: flex_depl) [leave blank and you will be asked for] Nothing more to clarify. Password is written to apply.env which is mentioned in .gitignore Enter path to depot [_depot] This is a relative path which points to the depot (artifactory) and is also mentioned in .gitignore when it is not starting with \"..\" Enter stage of this configuration mapped to branch (develop, test, master) [develop] When importing the deployment, this setting assigns the database connection to the source branch Do you wish to generate and install default tooling? (Logger, utPLSQL, teplsql, tapi) [Y] Here you activate the initial installation of the different components/dependencies. These are placed in the db/_setup/features folder. There you can also place other features by yourself. If you don't need certain components, you can delete the corresponding file from the features folder (before running the actual installation).. Install with sql(cl) or sqlplus? [sqlplus] Here you define which CLI dbFLow should use to execute the SQL scripts. Enter application IDs (comma separated) you wish to use initialy (100,101,...) Here you can already enter the application IDs that dbFlow should initially take care of Enter restful Moduls (comma separated) you wish to use initialy (api,test,...) Here you can already specify the REST modules that dbFlow should initially take care of After answering all the questions, your project structure is created with some SQL and bash files inside. You can now modifiy these files and / or put some files of your own into the corresponding folders. When you are ready, just install the project with dependencies into your database $ .dbFlow/setup.sh install This will run all SQL and bash files inside the db/_setup directory. For deployment or release purposes. This is the time to make all branches equal. So if you are on master and this your only branch, create a branch for each deployment stage (develop, test, ...) or if you allready did that, just merge these changes into your branches. Now the actual development work may start. After that have a look to the tutorial on How to make a release","title":"How to setup"},{"location":"getting_started/#getting-startet","text":"","title":"Getting startet"},{"location":"getting_started/#requirements","text":"To use dbFlow, all you need is: Git SQLPlus inkl. Oracle Client or SQLcl bash on Windows included as git-bash on MacOS included as bash (old) or zsh (unstable) on Linux it just should work","title":"Requirements"},{"location":"getting_started/#installation","text":"Basically, installing dbFLow consists of nothing more than initializing a directory with git and then cloning the repository as a submodule from GitHub. If you are using an existing git directory / project, cloning the submodule is all you need to do. The cloning of dbFlow as a submodule has the advantage that you can source new features and bugfixes directly from Github. In addition, you always have the possibility to fall back on an older dbFLow version through branching. Warning dbFlow MUST exist as a folder named \".dbFlow\"","title":"Installation"},{"location":"getting_started/#starting-a-new-project","text":"# create a folder for your project and change directory into $ mkdir demo && cd demo # init your project with git $ git init # clone dbFlow as submodule $ git submodule add https://github.com/MaikMichel/dbFlow.git .dbFlow","title":"Starting a new Project"},{"location":"getting_started/#initialize-an-existing-project","text":"# change into your project directory $ cd demo # clone dbFlow as submodule $ git submodule add https://github.com/MaikMichel/dbFlow.git .dbFlow Sometimes it can happen that the bash files cannot be executed. If this is the case, explicit permissions must be granted here. ( chmod +x .dbFlow/*.sh )","title":"Initialize an existing Project"},{"location":"getting_started/#setting-up-a-project","text":"To configure a dbFlow project, set it up initially with the command setup.sh generate <project_name> . After that some informations about the project are expected. All entries are stored in the two files build.env and apply.env . build.env contains information about the project itself and apply.env contains information about the database connection and environment. The filename apply.env is stored in the file .gitignore and should not be versioned. Since this file contains environment related information, it is also exactly the one that changes per instance / environment. All settings in this file can be entered also from \"outside\", in order to fill these for example from a CI/CD Tool. Info You can always execute a main shell script/command inside .dbFLow folder to show usage help.","title":"Setting up a project"},{"location":"getting_started/#generate-project","text":"$ .dbFlow/setup.sh generate <project_name> Question Notes Would you like to have a single, multi or flex scheme app (S/M/F) [M] This is the first question. Here you define the project mode. Default is M ulti When running release tests, what is your prefered branch name [build] Later you have the possibility to run so called release tests (NightlyBuilds) . Here you determine the branch name for the tests. The default here is build . Would you like to process changelogs during deployment [Y] dbFlow offers the possibility to generate changlogs based on the commit messages. Here you activate this function. changelog What is the schema name the changelog are processed with [schema_name] WIf you want changelogs to be displayed within your application, you can specify the target schema here, with which the corresponding TemplateCode should be executed. changelog Enter database connections [localhost:1521/xepdb1] Place your connection string like: host:port/service Enter username of admin user (admin, sys, ...) [sys] This user is responsible for all scripts inside db/_setup folder Enter password for sys [leave blank and you will be asked for] Nothing more to clarify. Password is written to apply.env which is mentioned in .gitignore Enter password for deployment_user (proxyuser: flex_depl) [leave blank and you will be asked for] Nothing more to clarify. Password is written to apply.env which is mentioned in .gitignore Enter path to depot [_depot] This is a relative path which points to the depot (artifactory) and is also mentioned in .gitignore when it is not starting with \"..\" Enter stage of this configuration mapped to branch (develop, test, master) [develop] When importing the deployment, this setting assigns the database connection to the source branch Do you wish to generate and install default tooling? (Logger, utPLSQL, teplsql, tapi) [Y] Here you activate the initial installation of the different components/dependencies. These are placed in the db/_setup/features folder. There you can also place other features by yourself. If you don't need certain components, you can delete the corresponding file from the features folder (before running the actual installation).. Install with sql(cl) or sqlplus? [sqlplus] Here you define which CLI dbFLow should use to execute the SQL scripts. Enter application IDs (comma separated) you wish to use initialy (100,101,...) Here you can already enter the application IDs that dbFlow should initially take care of Enter restful Moduls (comma separated) you wish to use initialy (api,test,...) Here you can already specify the REST modules that dbFlow should initially take care of After answering all the questions, your project structure is created with some SQL and bash files inside. You can now modifiy these files and / or put some files of your own into the corresponding folders.","title":"Generate Project"},{"location":"getting_started/#when-you-are-ready-just-install-the-project-with-dependencies-into-your-database","text":"$ .dbFlow/setup.sh install This will run all SQL and bash files inside the db/_setup directory. For deployment or release purposes. This is the time to make all branches equal. So if you are on master and this your only branch, create a branch for each deployment stage (develop, test, ...) or if you allready did that, just merge these changes into your branches. Now the actual development work may start. After that have a look to the tutorial on How to make a release","title":"When you are ready, just install the project with dependencies into your database"},{"location":"migrating/","text":"Migrating Basic adjustments If you want to migrate from an old version to a new version, you have to do the following steps pull the current dbFlow version from github Adjust the directory structure from top to bottom Important This means that you have to make the changes in the master branch and merge them through the stage branches to the development branch. This way, there is no new delta to be imported for these adjustments in the next release. # after the changes $...@master: git commit -m \"migrating dbFlow new structure\" $...@master: git push # checkout user acceptence stage $...@master: git checkout uac $...@uac: git merge master $...@uac: git push # checkout user test stage $...@uac: git checkout test $...@test: git merge uac $...@test: git push # checkout user development stage $...@test: git checkout develop $...@develop: git merge test $...@develop: git push Following changes in directory structure from 0.9.8 - stable to 1.0.0 The folder tables_ddl becomes a subfolder of the folder tables . The directories dml/pre and dml/post , and ddl/pre and ddl/post become: dml/patch/pre and dml/patch/post , and ddl/patch/pre and ddl/patch/post . In old versions there was the folder source . This is treated as plural now and must be renamed to sources . Following changes in build.env from 0.9.8 - stable to 1.0.0 have to be done You have to store project mode inside build.env # Following values are valid: SINGLE, MULTI or FLEX PROJECT_MODE = MULTI When you want to use release.sh for nightlybuild, you have to name the build branch # Name of the branch, where release tests are build BUILD_BRANCH = build When you are using the generate changelog feature you have to adjust the following vars # Generate a changelog with these settings # When template.sql file found in reports/changelog then it will be # executed on apply with the CHANGELOG_SCHEMA . # The changelog itself is structured using INTENT_PREFIXES to look # for in commits and to place them in corresponding INTENT_NAMES inside # the file itself. You can define a regexp in TICKET_MATCH to look for # keys to link directly to your ticketsystem using TICKET_URL CHANGELOG_SCHEMA = schema_name INTENT_PREFIXES =( Feat Fix ) INTENT_NAMES =( Features Fixes ) INTENT_ELSE = \"Others\" TICKET_MATCH = \"[A-Z]\\+-[0-9]\\+\" TICKET_URL = \"https://url-to-your-issue-tracker-like-jira/browse\"","title":"Migrating from previous"},{"location":"migrating/#migrating","text":"","title":"Migrating"},{"location":"migrating/#basic-adjustments","text":"If you want to migrate from an old version to a new version, you have to do the following steps pull the current dbFlow version from github Adjust the directory structure from top to bottom Important This means that you have to make the changes in the master branch and merge them through the stage branches to the development branch. This way, there is no new delta to be imported for these adjustments in the next release. # after the changes $...@master: git commit -m \"migrating dbFlow new structure\" $...@master: git push # checkout user acceptence stage $...@master: git checkout uac $...@uac: git merge master $...@uac: git push # checkout user test stage $...@uac: git checkout test $...@test: git merge uac $...@test: git push # checkout user development stage $...@test: git checkout develop $...@develop: git merge test $...@develop: git push","title":"Basic adjustments"},{"location":"migrating/#following-changes-in-directory-structure-from-098-stable-to-100","text":"The folder tables_ddl becomes a subfolder of the folder tables . The directories dml/pre and dml/post , and ddl/pre and ddl/post become: dml/patch/pre and dml/patch/post , and ddl/patch/pre and ddl/patch/post . In old versions there was the folder source . This is treated as plural now and must be renamed to sources .","title":"Following changes in directory structure from 0.9.8 - stable to 1.0.0"},{"location":"migrating/#following-changes-in-buildenv-from-098-stable-to-100-have-to-be-done","text":"You have to store project mode inside build.env # Following values are valid: SINGLE, MULTI or FLEX PROJECT_MODE = MULTI When you want to use release.sh for nightlybuild, you have to name the build branch # Name of the branch, where release tests are build BUILD_BRANCH = build When you are using the generate changelog feature you have to adjust the following vars # Generate a changelog with these settings # When template.sql file found in reports/changelog then it will be # executed on apply with the CHANGELOG_SCHEMA . # The changelog itself is structured using INTENT_PREFIXES to look # for in commits and to place them in corresponding INTENT_NAMES inside # the file itself. You can define a regexp in TICKET_MATCH to look for # keys to link directly to your ticketsystem using TICKET_URL CHANGELOG_SCHEMA = schema_name INTENT_PREFIXES =( Feat Fix ) INTENT_NAMES =( Features Fixes ) INTENT_ELSE = \"Others\" TICKET_MATCH = \"[A-Z]\\+-[0-9]\\+\" TICKET_URL = \"https://url-to-your-issue-tracker-like-jira/browse\"","title":"Following changes in build.env from 0.9.8 - stable to 1.0.0 have to be done"},{"location":"tutorial/","text":"A Tutorial Intro What will we build here small ToDo App incl. REST connection sketch of the architecture preferably a diagram reference to dockawex Configuration Examples","title":"A Tutorial"},{"location":"tutorial/#intro","text":"What will we build here small ToDo App incl. REST connection sketch of the architecture preferably a diagram reference to dockawex","title":"Intro"},{"location":"tutorial/#configuration","text":"","title":"Configuration"},{"location":"tutorial/#examples","text":"","title":"Examples"}]}