{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"The Deployment framework for Oracle Database and APEX Applications"},{"location":"concept/","title":"The Concept","text":"<p>dbFlow is based on 4 core topics. These topics are encountered again and again during the work with dbFLow. One is the adaptation of the git flow related to the work with database changes, the 2 phase deployment, smartFS, which reflects the directory structure and last but not least the depot, a kind of artifactory.</p>"},{"location":"concept/#git-flow-to-db","title":"Git Flow to DB","text":"<p>dbFlow is a deployment framework to deploy Oracle based projects / applications. dbFlow is roughly based on the so called Git Flow. This flow describes how releases of an application are delivered from the main development branch to the master branch, production. The following figure outlines this flow.</p>"},{"location":"concept/#git-flow-in-short","title":"Git Flow in short","text":"<p>Development mainly takes place on the develop branch. Features that are not included in the next release are developed in so-called feature branches. If a release has to be built from the current development state, the develop branch is merged into the release branch. This represents at the same time a so-called distribution boundary. This means that development can continue on the development branch and does not have to wait for the actual release. The release is first merged into the next higher branch. In our case this is the test branch. If an error is found here, it is fixed in the release branch and merged back to test. These changes also flow back into the development branch later. If the tests on the test stage are successful, the next branch is merged and the release is tested accordingly. This continues until the respective release has reached the master branch and therefore has reached production.</p>"},{"location":"concept/#reflect-db-changes","title":"Reflect DB changes","text":"<p>dbFlow supports the developers and release managers by building an artifact corresponding to the delta of 2 commits. Meaning the changed files, which can be applied to the respective database instance. So whenever you build a patch, you can refer to two commits as start and endpoint of a patch. Those commits can consist of state before and after merging ( HEAD, ORIG_HEAD), commit hashes or tags.</p> <p></p> <p>For each target branch a separate database or database container is expected. The artifacts that dbFLow generates can then be installed into the databases/containers.</p>"},{"location":"concept/#2-phase-deployment","title":"2 Phase deployment","text":"<p>dbFlow can generate two types of artifacts, which can be applied to the databases. On the one hand this is a so-called INITial deployment, which clears the target schemas first, or expects empty schemas, and on the other hand a PATCH deployment, which requires the corresponding previous version of a deployment on the database.</p> <p>Info</p> <p>Usually, the initial deployment is only installed ONCE on a production system. After that, only patch versions are applied.</p> <p>The advantage of this two-phase approach is that a version of an application can always be built directly and thus forms the basis of a subsequent patch. For example, one can build the initial version of a predecessor and install the current state as a patch on top of it. This procedure forms the basis of flawless releases.</p> <p></p> <p>With this approach it is easy to map the concept of NighlyBuilds with Jenkins or similar CI/CD tools. To test whether a release can be applied to Prod/Master, it is sufficient to create an initial release of Master and apply it to a test database. Afterwards, the patch, in other words the delta to the development branch, is applied.</p> <p>Important</p> <ul> <li>In INIT mode, all files from all directories except those named with patch are applied.</li> <li>In PATCH mode only the changed files are applied.</li> </ul> <p>Files inside .hook - folders will be always executed</p>"},{"location":"concept/#smartfs","title":"SmartFS","text":"<p>SmartFolderStructure</p> <p>To support this concept, a dbFlow project must have a certain directory structure. dbFLow expects the following directories at the root level of the project itself.</p> Folder Description apex All APEX application will be stored in this folder db This folder contains the required database schemas and their objects. rest Here the REST services / modules are stored reports binary files for templating purpose static In this folder the files are stored, which you upload later with the upload command to the static files of the respective application. (managed by <code>dbFlux</code>)"},{"location":"concept/#project-types","title":"Project Types","text":"<p>This structure is independent from the project-type. dbFlow knows 3 types of projects: SingleSchema, MultiSchema and FlexSchema</p> <p></p> <p><code>SingleSchema</code> and <code>MultiSchema</code> are very similar. APEX applications and REST modules are imported or exported with the application user. The name of the workspace is stored in both modes in the project configuration. In <code>MultiSchema</code> mode two additional schemas (LOGIC and DATA) are used. Here a three layer model is built up, which holds the data in the DATA schema, the business logic in the LOGIC schema and everything that is application specific in the APP schema. In <code>SingleSchema</code> mode, a direct database connection is established with the respective schema. In <code>MultiSchema</code> mode, as well as in <code>FlexSchema</code> mode, a ProxyUser is used for the database connection, which then connects to the target schema. If the project is configured in <code>FlexMode</code>, you can freely configure the corresponding schemas and workspaces via the directory structure. When using <code>FlexMode</code>, APEX applications are stored within workspace folders, which are again located within schema folders. The same applies to the static subdirectories, of course. REST modules are also stored in their own schema folder in <code>FlexMode</code>. Importing or exporting is done via the proxy user, which connects to the respective schema, which is configured by directory name.</p>"},{"location":"concept/#schema-folders","title":"Schema Folders","text":"<p>Each schemafolder inside db folder except <code>_setup</code> is build with the same structur.</p> Folder Description db _setup This folder contains the required objects your application depends on. All scripts inside are called by the preferred admin account you configured (sys, admin, ...) .. schema .... constraints Constraints are stored here and subdivided according to their type ...... checks Check Constraints ...... foreigns Foreign Keys ...... primaries Primary Keys ...... uniques Unique Keys .... contexts Sys_Contexts when needed .... ddl DDL Scripts for deployment subddivided on deployment mode ...... init Init scripts are executed only for the first installation (init). ...... patch Patch scripts are executed only in the update case (patch) and are divided into scripts that are executed at the beginning or at the end of the respective update. ........ post ........ pre .... dml DML Scripts for deployment subddivided on deployment mode ...... base Base scripts are always executed, no matter in what mode the deployment is used (first install - init) or (update - patch). Therefore they must be restartable. ...... init Init scripts are executed only for the first installation (init). ...... patch Patch scripts are executed only in the update case (patch) and are divided into scripts that are executed at the beginning or at the end of the respective update. ........ post ........ pre .... indexes Indexes are stored here and subdivided according to their type ...... defaults Non uniqe indexes ...... primaries Unique Indexes based in primary key columns ...... uniques Unique Indexes .... jobs Jobs, Scheduler scripts goes here .... policies Policies .... sequences Sequences must be scripted in a restartable manner .... sources All PL/SQL Code is stored in respective subfolders ...... functions ...... packages Extension for package specification is pks and extension pkb is used for body ...... procedures ...... triggers ...... types .... views Views goes here .... tables Here are all create table scripts stored ...... tables_ddl All table alter or modification scripts named with tablename.num.sql goes here .....tests Unittests ...... packages Packages containing utPLSQL Unittests <p>All files must be stored as executable SQL scripts. dbFlow uses SQLPlus or SQLcl to import these scripts.</p> <p>Warning</p> <p>Don't forget the trailing slashes in PLSQL files</p>"},{"location":"concept/#installation-sequence","title":"Installation Sequence","text":"<p>The file structure has several purposes. On the one hand, these directories represent a fixed point in time during deployment, i.e. they reflect the order of execution. For example, tables are applied before the indexes and the constraints. Primary key constraints are applied before foreign key constraints, and so on.</p> <p>In addition, the division of the table directory into tables and tables_ddl represents a kind of toggle. In case of an initial delivery only the scripts from the table directory are executed. If it is a patch delivery, the script from the table directory will not be executed if files with the same root name exist in the tables_ddl directory.</p> <p>For example, if the table employees changes, the file is updated accordingly with the actual create-table statement. In addition, a matching alter-table script with the same base name is stored in the tables_ddl directory.</p> <p><pre><code>// employees.sql\ncreate table employees (\nemp_id        number not null,\nemp_name      varchar2(250 char) not null,\nemp_birthday  date                        -- new column\n);\n</code></pre> <pre><code>// employees.1.sql\nalter table employees add (\nemp_birthday  date                        -- new column\n);\n</code></pre></p> files content tables - table_ddl - - employees.1.sql alter table add (..); - employees.sql create table (..); <p>Tip</p> <ul> <li>Changes to tables are always made 2 times. Once for a new installation (init) and once for an update (patch).</li> <li>Table scripts may only contain the pure tables and comments. Constraints and indexes must be stored in the appropriate directories.</li> <li>Files from the table directory are executed if it is a new installation (init) or if the table is to be newly created in this update (patch).</li> <li>Within the directories the files to be imported are sorted alphabetically, inside pacages or types all files are sorted alphabetically too but grouped in chunke specs, bodies, sql-files directory.</li> </ul>"},{"location":"concept/#init","title":"init","text":"<p>The following schema directories are applied in exactly this order during an init deployment:</p> <ul> <li>.hooks/pre</li> <li>sequences</li> <li>tables</li> <li>indexes/primaries</li> <li>indexes/uniques</li> <li>indexes/defaults</li> <li>constraints/primaries</li> <li>constraints/foreigns</li> <li>constraints/checks</li> <li>constraints/uniques</li> <li>contexts</li> <li>policies</li> <li>sources/types</li> <li>sources/packages</li> <li>sources/functions</li> <li>sources/procedures</li> <li>views</li> <li>mviews</li> <li>sources/triggers</li> <li>jobs</li> <li>tests/packages</li> <li>ddl</li> <li>ddl/init</li> <li>dml</li> <li>dml/init</li> <li>dml/base</li> <li>.hooks/post</li> </ul>"},{"location":"concept/#patch","title":"patch","text":"<p>The following schema directories are applied in exactly this order during a patch deployment:</p> <ul> <li>.hooks/pre</li> <li>ddl/patch/pre_${branch-name}</li> <li>dml/patch/pre_${branch-name}</li> <li>ddl/patch/pre</li> <li>dml/patch/pre</li> <li>sequences</li> <li>tables</li> <li>tables/tables_ddl</li> <li>indexes/primaries</li> <li>indexes/uniques</li> <li>indexes/defaults</li> <li>constraints/primaries</li> <li>constraints/foreigns</li> <li>constraints/checks</li> <li>constraints/uniques</li> <li>contexts</li> <li>policies</li> <li>sources/types</li> <li>sources/packages</li> <li>sources/functions</li> <li>sources/procedures</li> <li>views</li> <li>mviews</li> <li>sources/triggers</li> <li>jobs</li> <li>tests/packages</li> <li>ddl</li> <li>ddl/patch/post_${branch-name}</li> <li>dml/patch/post_${branch-name}</li> <li>ddl/patch/post</li> <li>dml</li> <li>dml/base</li> <li>dml/patch/post</li> <li>.hooks/post</li> </ul>"},{"location":"concept/#hooks","title":"Hooks","text":"<p>On each level of the main directories there are so called <code>.hooks</code> folders. You can find them in the root directory, in the db folder and in the schema folders. The <code>.hooks</code> folders are always divided into the subfolders <code>pre</code> and <code>post</code>. During the deployment process the scripts there will be executed in alphabetically order. These type of folders are meant to hold scripts which won't change too much during lifecycle of your product. For example you could place some kind of generator script inside here (Gen TableAPI, ...).</p> <p>Note</p> <p>Hookscripts outside the respective schema folders must contain the corresponding target schema in the name.</p> <pre><code>\u251c\u2500 .hooks\n\u2502  \u251c\u2500 pre\n\u2502  \u2502  \u251c\u2500 init\n\u2502  \u2502  \u251c\u2500 patch\n\u2502  \u2502  \u2502  \u2514\u2500 01_schema_a_do_something_in_patch_mode.sql\n\u2502  \u2502  \u251c\u2500 01_schema_a_do_something.sql\n\u2502  \u2502  \u2514\u2500 02_schema_a_do_something.sql\n\u2502  \u2514\u2500 post\n\u2502     \u251c\u2500 01_schema_c_do_something.sql\n\u2502     \u251c\u2500 02_schema_b_do_something.sql\n\u2502     \u2514\u2500 03_schema_b_do_something.sql\n\u251c\u2500 apex\n\u251c\u2500 db\n\u2502  \u251c\u2500 .hooks\n\u2502  \u2502  \u251c\u2500 pre\n\u2502  \u2502  \u2502  \u251c\u2500 init\n\u2502  \u2502  \u2502  \u2514\u2500 patch\n\u2502  \u2502  \u2514\u2500 post\n\u2502  \u2502     \u251c\u2500 init\n\u2502  \u2502     \u2514\u2500 patch\n\u2502  \u251c\u2500 schema_a\n\u2502  \u2514\u2500 schema_b\n\u2502     \u251c\u2500 .hooks\n\u2502     \u2502  \u251c\u2500 pre\n\u2502     \u2502  \u2502  \u2514\u2500 init\n\u2502     \u2502  \u2502  \u2514\u2500 patch\n\u2502     \u2502  \u251c\u2500 post\n\u2502     \u2502  \u2502  \u2514\u2500 init\n\u2502     \u2502  \u2502  \u2514\u2500 patch\n\u2502     \u2514\u2500 ...\n\u251c\u2500 rest\n\u251c\u2500 static\n</code></pre> <p>To execute a hook on a specific object type, you can add this structure to the corresponding directory within the schema hook. In this case you won't have the ability to diffentiate between the to modes.</p> <pre><code>\u251c\u2500 db\n\u2502  \u2514\u2500 schema_a\n\u2502     \u251c\u2500 .hooks\n\u2502     \u2502  \u251c\u2500 pre\n\u2502     \u2502  \u2514\u2500 post\n\u2502     \u2502     \u2514\u2500 sources\n\u2502     \u2502        \u2514\u2500 packages\n\u2502     \u2502           \u2514\u2500 call_me_after_packages_applied.sql\n\u2502     \u2514\u2500 sources\n\u2502        \u2514\u2500 packages\n\u2502           \u251c\u2500 my_package.pks\n\u2502           \u2514\u2500 my_package.pkb\n</code></pre>"},{"location":"concept/#each-table-hook","title":"Each-Table-Hook","text":"<p>Since version 2.1.0 there is the option to provide schema hooks with the suffix <code>.tables.sql</code>. dbFlow will then call this script for all tables in init mode or only for the changed tables in patch mode. In these cases the respective script is called with 3 parameters.</p> <ol> <li>Version - Name of the version during install &gt; <code>1.0.1</code></li> <li>Mode    - One of known modes &gt; <code>init</code> | <code>patch</code></li> <li>Table   - Name of the table with sql as extension &gt; <code>employees.sql</code></li> </ol> <p>Tip</p> <ul> <li>You have to clear the used SQLplus/SQLcl variables yourself.</li> <li>I recommend to write the script in a way that it handles all tables when called without parameters. So you could handle an array of tables in the script that, if the parameter with the table name is set accordingly, handles only one table, otherwise all.</li> </ul> <p>Example each-table-hook script <code>db/example_schema/.hooks/post/010_ge_example_output.tables.sql</code>:</p> <pre><code>set define '^'\nset concat on\nset concat .\nset verify off\nset serveroutput on\nset linesize 2000\nset wrap off\nset trimspool on\nset termout off\n\nCOLUMN 1 NEW_VALUE 1\nCOLUMN 2 NEW_VALUE 2\nCOLUMN 3 NEW_VALUE 3\n\nselect '' \"1\" from dual where rownum = 0;\nselect '' \"2\" from dual where rownum = 0;\nselect '' \"3\" from dual where rownum = 0;\n\ndefine _PARAMETER_01 = ^1 \"0.0.0\"\ndefine _PARAMETER_02 = ^2 \"undefined\"\ndefine _PARAMETER_03 = ^3 \"ALL_TABLES\"\n\ndefine VERSION    = ^_PARAMETER_01\ndefine MODE       = ^_PARAMETER_02\ndefine ALL_TABLES = ^_PARAMETER_03\n\nset termout on\nset timing on;\n--\n-- place your api logic here\n\nprompt Example output for ^ALL_TABLES in version: ^VERSION and mode: ^MODE\n\n-- end api logic\n--\nRem undefine all\n\nundefine 1;\nundefine 2;\nundefine 3;\n\nundefine _PARAMETER_01;\nundefine _PARAMETER_02;\nundefine _PARAMETER_03;\n\nundefine VERSION;\nundefine MODE;\nundefine ALL_TABLES;\n</code></pre>"},{"location":"concept/#apex-applications","title":"APEX Applications","text":"<p>Applications are stored in the apex directory. The applications will be expected in splitted form. For each application there is a corresponding folder containing the respective files (standard APEX export). For a deployment only the appropriate changes are included. A configuration option can be used here, in order to include all files into the deployment.</p> <pre><code>\u251c\u2500 apex\n\u2502  \u2514\u2500 f1000\n\u2502  \u2502  \u251c\u2500 application\n\u2502  \u2502  \u2514\u2500 install.sql\n\u2502  \u2514\u2500 f2000\n</code></pre> <p>In SingleSchema and MultiSchema mode, the workspace is stored in the project configuration. The target schema is always the APP schema. If you use FlexSchema mode, however, this information is stored in the directory tree itself.</p> <pre><code>\u251c\u2500 apex\n\u2502  \u2514\u2500 schema_a\n\u2502  \u2502  \u2514\u2500 workspace_x\n\u2502  \u2502     \u2514\u2500 f1000\n\u2502  \u2514\u2500 schema_b\n\u2502     \u2514\u2500 workspace_y\n\u2502        \u2514\u2500 f2000\n</code></pre>"},{"location":"concept/#rest-modules","title":"REST Modules","text":"<p>If you use REST modules in your project, they are placed in the rest folder. Here dbFlow also expects a certain directory structure. This has a direct influence on the order of execution during the deployment.</p> <pre><code>\u251c\u2500 rest\n\u2502  \u251c\u2500 access\n\u2502  \u2502  \u251c\u2500 roles\n\u2502  \u2502  \u251c\u2500 privileges\n\u2502  \u2502  \u2514\u2500 mapping\n\u2502  \u2514\u2500 module\n\u2502     \u251c\u2500 module_name_a\n\u2502     \u2502  \u2514\u2500 module_name_a.module.sql\n\u2502     \u2514\u2500 module_name_b\n\u2502        \u251c\u2500 module_name_b.module.sql\n\u2502        \u2514\u2500 module_name_b.condition.sql\n</code></pre> <p>Files are imported into these folders in the following order.</p> <ol> <li>access/roles</li> <li>access/privileges</li> <li>access/mapping</li> <li>modules</li> </ol> <p>Within the folders the order is alphabetical. If dbFLow finds a module file with the same file base and the extension *.condition.sql, this condition will be included in the install script.</p> <p>Important</p> <p>In the Condtion file a PLSQL expression is expected which returns a boolean value. Later this expression becomes part of an IF condition. In order to work that way the module file itself has to exclude then trailing slash on the last line!</p>"},{"location":"concept/#depot","title":"Depot","text":"<p>When you create a deployment, whether INIT or PATCH, the deployment artifact is stored in a so-called depot. From this depot, a CI/CD tool, for example, can later fetch the artifact and install it on the target instance. Within the depot directory, the individual deployments are stored in subdirectories that correspond to the Git branch from which they were created.</p> <p></p> <p>I recommend to use a separate repository or directory for each stage DB. This has the advantage that the corresponding directories serve their purpose even without Git and possibly access to the development repository. Theoretically, the repository can also be \"doubled\" to have a target directory at home and a source directory at the customer.</p>"},{"location":"concept/#logging","title":"Logging","text":"<p>After a successful installation, all logs and temporary created artifacs, are placed in a log folder. Errors during the deployment lead to an abort and are stored in the failures subfolder. The location of the log folder itself is configured in apply.env.</p> <p>We recommend using the actual instance folder to ensure greater transparency.</p>"},{"location":"concept/#big-picture","title":"Big Picture","text":""},{"location":"deployment/","title":"Deployment","text":"<p>As already described in the concept dbFlow knows two types of releases. One is the initial release, which imports the complete product against empty schemas or clears them first, and the other is the patch release, which determines the delta of two commit levels and applies it on top to an existing installation.</p> <p></p> <p>A deployment can be divided into two steps. In the first step, the artifact is built. This is done by the script <code>.dbFlow/build.sh</code>. In the second step the artifact is applied to the target environment. This is done by calling the script <code>.dbFlow/apply.sh.</code></p> <ul> <li>Step 1: build - This is where the artifact is created.</li> <li>Step 2: apply - This is where the artifact is deployed to the respective target environment.</li> </ul> <p>Warning</p> <p>Remember, during an init, the target schemas are cleared at the beginning. All objects will be dropped!</p>"},{"location":"deployment/#build","title":"build","text":"<p>The build.sh script is used to create the so-called build file or artifact. This is used to create the actual release based on the state of the files in the directory tree / Git repo. As with any dbFlow script, you can display the possible parameters by omitting the parameters or explicitly with the <code>--help</code> parameter.</p> <p>The build.sh script creates a tar ball with all relevant files and stores it in the depot. The location of the depot is determined in the file apply.env.</p>"},{"location":"deployment/#init","title":"init","text":"<p>Example: <code>.dbFlow/build.sh --init --version 1.0.0</code></p> <p>In an initial release, all files from the database directories are determined and imported in a specific order.</p> <p>By using the flag <code>-i/--init</code> an initial release is created. Additionally you need the target version of the release. With the flag <code>-v/--version</code> you name the version of the release. The files from the directories <code>\\*/[ddl|dml]/patch/\\*</code> and <code>\\*/tables/tables_ddl</code> are ignored. The release itself is created from the current branch. If you want to create a release from another branch, you have to switch there with Git first.</p> <p>Order of the directories</p> Num Folder Num Folder Num Folder 1 .hooks/pre 11 contexts 21 tests/packages 2 sequences 12 policies 22 ddl/init 3 tables 13 sources/types 23 dml/init 4 indexes/primaries 14 sources/packages 24 dml/base 5 indexes/uniques 15 sources/functions 25 .hooks/post 6 indexes/defaults 16 sources/procedures 7 constraints/primaries 17 views 8 constraints/foreigns 18 mviews 9 constraints/checks 19 sources/triggers 10 constraints/uniques 20 jobs <p>If you use the flag -a/--apply then the release will be directly applied to the current environment.</p>"},{"location":"deployment/#additional-arguments-in-init-mode","title":"Additional arguments in init mode:","text":"<ul> <li>keepFolder</li> </ul> <p>With the flag <code>-k / --keepfolder</code> the working directory, which is created in the depot to create the actual artifact, is not deleted. Especially in the beginning, when you don't have so much experience with dbFlow, this option is helpful. So after creating the artifact you can navigate to the corresponding directory and have a look at the created scripts and copied files.</p>"},{"location":"deployment/#patch","title":"patch","text":"<p>Example: <code>.dbFlow/build.sh --patch --version 1.1.0</code></p> <p>By using the flag <code>-p/--patch</code> a patch release is created. Additionally you need the target version of the release. With the flag <code>-v/--version</code> you name the version of the release.</p> <p>In a patch release all changed files are determined. These files then become part of the deployment. Which files are considered as modified is determined by Git. With the parameters <code>-s/--start</code> (defaults to ORIG_HEAD) and <code>-e/--end</code> (defaults to HEAD) one can set these parameters explicitly. Which files become part of the patch can be output by using the <code>-l/--listfiles</code> flag.</p> <p>Start should always be at least one commit prior to the end commit.</p> <p>The files from the directories <code>\\*/[ddl|dml]/init/\\*</code> are ignored. Additionally there is the import switch, which says, that if the table to be changed exists in the <code>table</code> folder AND in the <code>table_ddl</code> folder, ONLY the files with the same name from the <code>table_ddl</code> folder are imported.</p> <p>Order of the directories</p> Num Folder Num Folder Num Folder 01 .hooks/pre 12 constraints/primaries 23 mviews 02 ddl/patch/pre_${branch} 13 constraints/foreigns 24 sources/triggers 03 dml/patch/pre_${branch} 14 constraints/checks 25 jobs 04 ddl/patch/pre 15 constraints/uniques 26 tests/packages 05 dml/patch/pre 16 contexts 27 ddl/patch/post_${branch} 06 sequences 17 policies 28 dml/patch/post_${branch} 07 tables 18 sources/types 29 ddl/patch/post 08 tables/tables_ddl 19 sources/packages 30 dml/base 09 indexes/primaries 20 sources/functions 31 dml/patch/post 00 indexes/uniques 21 sources/procedures 32 .hooks/post 11 indexes/defaults 22 views <pre><code>$ .dbFlow/build.sh --patch --version 1.1.0\n$ .dbFlow/build.sh --patch --version 1.2.0 --start 1.0.0\n$ .dbFlow/build.sh --patch --version 1.3.0 --start 71563f65 --end ba12010a\n$ .dbFlow/build.sh --patch --version 1.4.0 --start ORIG_HEAD --end HEAD\n</code></pre> <p>For example, by using stage branches, you can merge the current state of the develop branch into the test branch and build the release by implicitly using the Git variables HEAD and ORIG_HEAD.</p> <pre><code># make shure you have all changes\n@develop$ git pull\n\n# goto branch mapped to test-Stage\n@develop$ git checkout test\n\n# again: make shure you have all changes\n@test$ git pull\n\n# merge all chages from develop\n@test$ git merge develop\n\n# build the relase artifact\n@test$ .dbFlow/build.sh --patch --version 1.2.0\n</code></pre>"},{"location":"deployment/#repair","title":"Repair","text":"<p>If you want to put something only in a certain stage, for example because something went wrong in a release, so you want to clean it up, you can put these scripts in the directories <code>ddl/patch/pre_${branch}</code> or <code>dml/patch/pre_${branch}</code>. Since the concept is that branches are assigned to stages, these scripts will only be executed in those stages.</p>"},{"location":"deployment/#additional-arguments-in-patch-mode","title":"Additional arguments in patch mode:","text":"<ul> <li>keepFolder</li> </ul> <p>With the flag <code>-k / --keepfolder</code> the working directory, which is created in the depot to create the actual artifact, is not deleted. Especially in the beginning, when you don't have so much experience with dbFlow, this option is helpful. So after creating the artifact you can navigate to the corresponding directory and have a look at the created scripts and copied files.</p> <ul> <li>transferAll</li> </ul> <p>In order to always transsper/copy all files, you can specify the <code>-t / --transferall</code> flag. In this case not only the changed files but all files will be included in the patch. During the deployment only the changed files will be applied within the DB directory.</p> <ul> <li>listFiles</li> </ul> <p>The -l / --listfiles flag can be used to check which files would be included in the artifact. With this option no actual build takes place.</p> <ul> <li>apply</li> </ul> <p>The -a / --apply flag can be used to apply the created artifact immediatly in to the current environment after building the patch.</p>"},{"location":"deployment/#apply","title":"apply","text":"<p>The apply.sh command applies a release to the respective configured database. The artifact is fetched from the depot and unpacked into the appropriate directory. Afterwards the installation scripts are executed. The environment variable STAGE, from the file apply.env, is used to determine the source directory of a release. The build script stores the artifacts in a directory in a depot that contains the current branch name. The naming of the variable STAGE is now used to get the artifact matching the stage / database connection.</p> <p></p> <p>With this method there can be n instances, which all point to the same depot directory.</p> <p>Info</p> <p>I recommend to create a corresponding instance directory for each database and to version it as well. This directory should also contain a .dbFlow - submodule. It is sufficient to copy the files apply.env and build.env into the directory and to adjust the database connection and the stage. See: ../concept/#big-picture</p>"},{"location":"deployment/#init_1","title":"init","text":"<p>Warning</p> <p>Importing an init release leads to data loss!</p> <p>By specifying the <code>-i/--init</code> flag, an init release is retrieved from the depot and then applied. If no password is stored in the apply.env (this is recommended), it will be requested at the very beginning. Because on init then content of all included schemas will be deleted you are asked to proceed. When you provide an environment variable called <code>DBFLOW_JENKINS</code> with any value the question is skipped.</p> <pre><code>$ .dbFlow/apply.sh --init --version 1.0.0\n</code></pre> <p>CI/CD</p> <p>All environment variables that are used in dbFlow by sourcing the files build.env and apply.env can also be provided from the outside, i.e. without the file system. Especially if you use tools like Jenkins and Co, you can influence the configuration of the targetstage.</p>"},{"location":"deployment/#additional-arguments-in-init-mode_1","title":"Additional arguments in init mode:","text":"<ul> <li>noExtract</li> </ul> <p>When using <code>-n/--noextract</code> flag, dbFlow will not not move and extract then artifact from depot. This option can be used to extract files manually or use a allready extracted artifact.</p>"},{"location":"deployment/#patch_1","title":"patch","text":"<p>By specifying the <code>-p/--patch</code> flag, a patch release is imported from the depot and then applied. If no password is stored in the apply.env (this is recommended), it will be requested at the very beginning.</p> <pre><code>$ .dbFlow/apply.sh --patch --version 1.1.0\n</code></pre> <p>After the installation of a release, all resulting log and installation files are stored in the configured <code>LOG_PATH</code> under a version directory and success or failure. Failure of course only if the installation was aborted due to an error.</p>"},{"location":"deployment/#additional-arguments-in-patch-mode_1","title":"Additional arguments in patch mode:","text":"<ul> <li>noExtract</li> </ul> <p>When using <code>-n/--noextract</code> flag, dbFlow will not not move and extract then artifact from depot. This option can be used to extract files manually or use a allready extracted artifact.</p> <ul> <li>redoLog</li> </ul> <p>Now it can happen that you want to continue the installation after an interruption at the same place. For this the parameter <code>-r/--redolog</code> is used. If you specify the log file of the previous installation, it will be analyzed and continued with the step that leads to an abort.</p> <pre><code>$ # get and copy the failed patch to instance folder\n$ cp ../depot/test/failure/patch_1.1.0.tar.gz ./\n$\n$ # apply patch by installing only the uncalled db scripts\n$ .dbFlow/apply.sh --patch --version 1.1.0 --redolog ./_log_path/test/failure/20230613121314_dpl_patch_1.1.0.log\n</code></pre>"},{"location":"deployment/#interactive-execution","title":"Interactive Execution","text":"<p>By using the option <code>-s/--stepwise</code> you are able to run the execution stepwise. Before each section of execution you will be asked to proceed.</p> <p>Note</p> <p>Keep in mind that you have to implement the fixes by your own. dbFlow won't reverse anything. When you want to clean you accidental applied files, you have to ship a cleanup script and put it in your stage folders: <code>ddl/patch/pre_${branch}</code> or <code>dml/patch/pre_${branch}</code></p>"},{"location":"deployment/#release","title":"release","text":"<p>Most of the time we do a patch release. So in these cases you have to:</p> <ol> <li>Change to the target branch</li> <li>Merge the source branch into it</li> <li>Build the patch</li> <li>If necessary tag the version in Git (optional to be implemented)</li> <li>Apply the version in the target environment (optional)</li> </ol> <p>You don't have to do these steps manually if you use the <code>.dbFlow/release.sh</code> script. Here you can simply specify your target branch and the script does the appropriate steps to merge the respective branches itself. By omitting the parameters you can also see what is needed.</p> <p>release.sh does the handling / merging of the branches for you and calls build.sh afterwards.</p> <pre><code>  .dbFlow/release.sh --target master --version 1.2.3\n</code></pre> <p>To do a release test, the release script can build three artifacts for you (flag <code>-b/--build</code>). This functionality is for the so-called nightly builds. Here an initial release of the predecessor, a patch of the successor, and an initial release of the successor are built. Using a CI/CD server, such as Jenkins, you can then create these 3 artifacts and install them one after the other on the corresponding instance and of course test them.</p> <pre><code>.dbFlow/release.sh --source develop --target master -b\n</code></pre>"},{"location":"deployment/#additional-arguments-for-release","title":"Additional arguments for release:","text":"<ul> <li>build</li> </ul> <p>As described above, the <code>-b/--build</code> flag creates 3 artifacts. These represent the current initial release, the previous initial release and the current patch release. All 3 artifacts can then be imported into the target environment in the appropriate order.</p> <ul> <li>apply</li> </ul> <p>With the option <code>-a/--apply</code> and the following argument of the instance directory, the release or releases (nightly builds) will be applied directly to the target instance.</p> <pre><code>$ .dbFlow/release.sh --source develop --target master --build --apply ../instances/nightly\n</code></pre> <ul> <li>keep</li> </ul> <p>The <code>-k/--keep</code> option prevents the working folders in the depot directory from being deleted.</p>"},{"location":"faq/","title":"Frequently Asked Questions or Tips","text":""},{"location":"faq/#how-can-i-correct-an-existing-error-retrospectively","title":"How can I correct an existing error retrospectively?","text":"<p>Sometimes it happens that an error slips in. This can happen, for example, if you forget to test the current version of the initial deployment (<code>init</code>). As soon as this happens and an error appears here, the subsequent deployments cannot build their predecessor cleanly. If something like this happens, then you have to merge the error correction from \"top to bottom\", i.e. from the production branch (master or main) down to the develop branch. This ensures that at the next deployment, the predecessor (e.g. version on test) can be built cleanly.</p> <p> </p>"},{"location":"faq/#proxyuser-connection-problems","title":"ProxyUser - connection problems","text":"<p>Sometimes it can happen that dbFlow or SQLplus or SQLcl cannot connect to the database. The error: <code>ORA-01017: invalid username/password; logon denied</code> is displayed. This may be due to the fact that in a SingleSchema the database directory does not match the value of the variable <code>DB_APP_USER</code> from the file <code>apply.env</code>. Please also check the notation here. dbFlow will ALWAYS, if the directory != DB_APP_USER, perform the login as proxy user to the database (<code>proxy_user[schema_name]@database:port</code>).</p>"},{"location":"features/","title":"Features","text":""},{"location":"features/#generating-changelogs","title":"Generating Changelogs","text":"<p>With dbFlow you have the possibility to create changelogs automatically. These are based on the commit messages of the commits that lead to a patch. The changelog file itself is kept as markdown. Additionally the changelog can be automatically processed (uploaded to the DB) during the deployment if there is a subdirectory named changelog in the reports-folder.</p>"},{"location":"features/#configuration","title":"Configuration","text":"<p>After creating the directory structure and environment files, the following variables were created in the build.env file.</p> <pre><code>CHANGELOG_SCHEMA=&lt;schema_name&gt;\nINTENT_PREFIXES=( Feat Fix )\nINTENT_NAMES=( Features Fixes )\nINTENT_ELSE=\"Others\"\nTICKET_MATCH=\"[A-Z]\\+-[0-9]\\+\"\nTICKET_URL=\"https://[your-jira-url]/browse\"\n</code></pre> Name Meaning CHANGELOG_SCHEMA With this scheme the changelog is processed by the reports directory INTENT_PREFIXES The commit messages are grouped with this array of prefixes INTENT_NAMES This array holds the labels of the prefixes, which then appear as headings in the actual changelog INTENT_ELSE All messages not handled by the prefixes appear here TICKET_MATCH RegularExpression to get the ticket number from the commit message TICKET_URL URL to which the ticket number is appended <p>A template.sql file is stored in the <code>reports/changelog</code> directory by default. This file contains the code you have to change to upload this changelog as a clob to a target table.</p> <p>For example like this:</p> <pre><code>  /***********************************************\n    -------     TEMPLATE START            -------\n    l_bin         &gt;&gt;&gt; Content as blob of file\n    l_file_name   &gt;&gt;&gt; name of the file\n                      changelog_init_1.2.3.md\n  ************************************************/\n\nDeclare\nl_version varchar2(100);\nBegin\nl_version := substr(l_file_name, instr(l_file_name, '_', 1, 2)+1);\nl_version := substr(l_version, 1, instr(l_version, '.', -1, 1)-1);\n\n-- START custom code\nbegin\ninsert into changelogs (chl_version, chl_date, chl_content)\nvalues (l_version, current_date, l_bin);\nexception\nwhen dup_val_on_index then\nupdate changelogs\nset chl_content = l_bin,\nchl_date    = current_date\nwhere chl_version = l_version;\nend;\n\n-- END custom code\n\ndbms_output.put_line(gc_green||' ... Version info uploaded: ' || l_version ||gc_reset);\nEnd;\n\n/***********************************************\n    -------     TEMPLATE END              -------\n  ************************************************/\n</code></pre>"},{"location":"features/#example","title":"Example","text":"<pre><code># Project XYZ - Changelog\n\n## Version 1.2.3 (2022-04-08)\n\n### Features\n\n* Changelogs and Version infos are now displayed\n* Emailfrom is included in ref_codes based on stage #XYZ-69 [View](https://your-own-jira-url-for-example.com/browse/XYZ-69)\n* Managing total revenues per corporations #XYZ-71 [View](https://your-own-jira-url-for-example.com/browse/XYZ-71)\n* Reporting Dashboard includes total revenues #XYZ-72 [View](https://your-own-jira-url-for-example.com/browse/XYZ-72)\n\n\n### Others\n\n* Add some unit tests\n* Application processes outsourced to package\n* Color Settings for VSCode\n* Fix Refresh Facetted Search after transaction occured\n* Refactored revenue_totals to be more testable\n* Set missing max length\n* Set plsql searchPath\n* XYZ-50: Changed Buttontext on confirm Email [View](https://your-own-jira-url-for-example.com/browse/XYZ-50)\n* XYZ-50: Update Email text of invitation [View](https://your-own-jira-url-for-example.com/browse/XYZ-50)\n* XYZ-74: Aliases in own table [View](https://your-own-jira-url-for-example.com/browse/XYZ-74)\n* XYZ-74: Nach Nachfrage aus bestehenden Supplier einen Alias machen [View](https://your-own-jira-url-for-example.com/browse/XYZ-74)\n</code></pre>"},{"location":"features/#switching-off-the-applications","title":"Switching off the applications","text":"<p>During installation all known APEX applications will be set to unavailable. This is done by APEX's own API apex_util.set_application_status. The content of the file .dbFlow/maintence.html will be displayed as a message to the end user. But you have the option to use your own maintence.html file. dbFlow will first try to read the apex/maintence.html file. If it is found, the content will be displayed as a message. Otherwise the one that exists in the .dbFlow directory.</p>"},{"location":"features/#example_1","title":"Example","text":"<pre><code>&lt;title&gt;Site Maintenance&lt;/title&gt;\n&lt;style&gt;\nbody { text-align: center; padding: 20px; }\n'||'@'||'media (min-width: 768px){\nbody{ padding-top: 200px; }\n}\nh1 { font-size: 50px; }\nbody { font: 20px Helvetica, sans-serif; color: #333; }\narticle { display: block; text-align: left; max-width: 650px; margin: 0 auto; }\na { color: #dc8100; text-decoration: none; }\na:hover { color: #333; text-decoration: none; }\n&lt;/style&gt;\n\n&lt;article&gt;\n    &lt;h1&gt;We&amp;rsquo;ll be back soon!&lt;/h1&gt;\n    &lt;div&gt;\n        &lt;p&gt;Sorry for the inconvenience but we&amp;rsquo;re performing some maintenance at the moment. We&amp;rsquo;ll be back online shortly!&lt;/p&gt;\n        &lt;p&gt;&amp;mdash; The Team&lt;/p&gt;\n    &lt;/div&gt;\n&lt;/article&gt;\n</code></pre>"},{"location":"features/#rest-modules","title":"REST modules","text":"<p>Additionally, all REST modules are also unpublished during an installation.</p>"},{"location":"features/#ms-teams-notification","title":"MS Teams Notification","text":"<p>After a deployment dbFlow is able to send a message to a Microsoft Teams hook. All you have to do is to provide the hook link set the variable TEAMS_WEBHOOK_URL.</p> <pre><code>TEAMS_WEBHOOK_URL=https://url-to-your-incoming-teams-webhook\n</code></pre>"},{"location":"migrating/","title":"Migrating","text":""},{"location":"migrating/#basic-adjustments","title":"Basic adjustments","text":"<p>If you want to migrate from an old version to a new version, you have to do the following steps</p> <ol> <li>pull the current dbFlow version from github</li> <li>Adjust the directory structure from top to bottom</li> </ol> <p>Important</p> <p>This means that you have to make the changes in the master branch and merge them through the stage branches to the development branch. This way, there is no new delta to be imported for these adjustments in the next release.</p> <pre><code>  # after the changes\n$...@master: git commit -m \"migrating dbFlow new structure\"\n$...@master: git push\n\n  # checkout user acceptence stage\n$...@master: git checkout uac\n  $...@uac: git merge master\n  $...@uac: git push\n\n  # checkout user test stage\n$...@uac: git checkout test\n$...@test: git merge uac\n  $...@test: git push\n\n  # checkout user development stage\n$...@test: git checkout develop\n  $...@develop: git merge test\n$...@develop: git push\n</code></pre>"},{"location":"migrating/#following-changes-in-directory-structure-from-098-stable-to-100","title":"Following changes in directory structure from 0.9.8 - stable to 1.0.0","text":"<ol> <li>The folder <code>tables_ddl</code> becomes a subfolder of the folder <code>tables</code>.</li> <li>The directories <code>dml/pre</code> and <code>dml/post</code>, and <code>ddl/pre</code> and <code>ddl/post</code> become:     <code>dml/patch/pre</code> and <code>dml/patch/post</code>, and <code>ddl/patch/pre</code> and <code>ddl/patch/post</code>.</li> <li>In old versions there was the folder <code>source</code>. This is treated as plural now and must be renamed to <code>sources</code>.</li> </ol>"},{"location":"migrating/#following-changes-in-buildenv-from-098-stable-to-100-have-to-be-done","title":"Following changes in build.env from 0.9.8 - stable to 1.0.0 have to be done","text":"<ol> <li>You have to store project mode inside build.env   <pre><code>  # Following values are valid: SINGLE, MULTI or FLEX\nPROJECT_MODE=MULTI\n</code></pre></li> <li> <p>When you want to use release.sh for nightlybuild, you have to name the build branch   <pre><code>  # Name of the branch, where release tests are build\nBUILD_BRANCH=build\n</code></pre></p> </li> <li> <p>When you are using the generate changelog feature you have to adjust the following vars   <pre><code>  # Generate a changelog with these settings\n# When template.sql file found in reports/changelog then it will be\n# executed on apply with the CHANGELOG_SCHEMA .\n# The changelog itself is structured using INTENT_PREFIXES to look\n# for in commits and to place them in corresponding INTENT_NAMES inside\n# the file itself. You can define a regexp in TICKET_MATCH to look for\n# keys to link directly to your ticketsystem using TICKET_URL\nCHANGELOG_SCHEMA=schema_name\n  INTENT_PREFIXES=( Feat Fix )\nINTENT_NAMES=( Features Fixes )\nINTENT_ELSE=\"Others\"\nTICKET_MATCH=\"[A-Z]\\+-[0-9]\\+\"\nTICKET_URL=\"https://url-to-your-issue-tracker-like-jira/browse\"\n</code></pre></p> </li> </ol>"},{"location":"setup/","title":"Getting startet","text":""},{"location":"setup/#requirements","title":"Requirements","text":"<p>To use dbFlow, all you need is:</p> <ul> <li>Git</li> <li>SQLPlus inkl. Oracle Client or SQLcl</li> <li>bash<ul> <li>on Windows included as git-bash</li> <li>on MacOS included as bash (old) or zsh (unstable)</li> <li>on Linux it just should work</li> </ul> </li> </ul>"},{"location":"setup/#installation","title":"Installation","text":"<p>Basically, installing dbFLow consists of nothing more than initializing a directory with git and then cloning the repository as a submodule from GitHub. If you are using an existing git directory / project, cloning the submodule is all you need to do.</p> <p>The cloning of dbFlow as a submodule has the advantage that you can source new features and bugfixes directly from Github. In addition, you always have the possibility to fall back on an older dbFLow version through branching.</p> <p>Important!</p> <p>dbFlow MUST exist as a folder named \".dbFlow\"</p>"},{"location":"setup/#starting-a-new-project","title":"Starting a new Project","text":"<pre><code># create a folder for your project and change directory into\n$ mkdir demo &amp;&amp; cd demo\n\n# init your project with git\n$ git init\n\n# clone dbFlow as submodule\n$ git submodule add https://github.com/MaikMichel/dbFlow.git .dbFlow\n</code></pre>"},{"location":"setup/#initialize-an-existing-project","title":"Initialize an existing Project","text":"<pre><code># change into your project directory\n$ cd demo\n\n# clone dbFlow as submodule\n$ git submodule add https://github.com/MaikMichel/dbFlow.git .dbFlow\n</code></pre>"},{"location":"setup/#or-with-a-one-liner","title":"Or with a One-Liner","text":"<pre><code># Without parameter current directory is used\ncurl -sS https://raw.githubusercontent.com/MaikMichel/dbFlow/master/install.sh | bash\n\n# Add targetfolder as parameter\ncurl -sS https://raw.githubusercontent.com/MaikMichel/dbFlow/master/install.sh | bash -s &lt;targetfolder&gt;\n</code></pre>"},{"location":"setup/#clone-an-existing-dbflow-project-multiple-steps","title":"Clone an existing dbFlow project (multiple steps)","text":"<pre><code># create project folder and change into\nmkdir your_project &amp;&amp; cd your_project\n\n# clone the repo itself\ngit clone https://path-to-your-db-flow-project-where-dbFlow-is-allready-installed.git .\n\n# pull submodule(s) =&gt; .dbFlow\ngit submodule update --init --recursive\n</code></pre>"},{"location":"setup/#clone-an-existing-dbflow-project-one-step","title":"Clone an existing dbFlow project (one step)","text":"<pre><code># clone the repo recursive and change into it\ngit clone --recursive https://path-to-your-db-flow-project-where-dbFlow-is-allready-installed.git your_project &amp;&amp; cd your_project\n</code></pre> <p>Sometimes it can happen that the bash files cannot be executed. If this is the case, explicit permissions must be granted here. (<code>chmod +x .dbFlow/*.sh</code>)</p>"},{"location":"setup/#setting-up-a-project","title":"Setting up a project","text":"<p>To configure a dbFlow project, set it up initially with the command <code>.dbFlow/setup.sh --generate &lt;project_name&gt;</code>. After that some informations about the project are expected. All entries are stored in the two files <code>build.env</code> and <code>apply.env</code>. <code>build.env</code> contains information about the project itself and <code>apply.env</code> contains information about the database connection and environment. The filename <code>apply.env</code> is stored in the file <code>.gitignore</code> and should not be versioned. Since this file contains environment related information, it is also exactly the one that changes per instance / environment. All settings in this file can be entered also from \"outside\", in order to fill these for example from a CI/CD Tool.</p> <p>Info</p> <p>You can always execute a bash script/command inside <code>.dbFLow</code> folder without arguments to show usage help.</p>"},{"location":"setup/#generate-project","title":"Generate Project","text":"<pre><code>$ .dbFlow/setup.sh --generate &lt;project_name&gt;\n</code></pre> <p>You will be asked the following questions when creating the project:</p> Question Notes Which dbFLow project type do you want to create? Single, Multi or Flex [M] This is the first question. Here you define the project mode. Default is Multi . ( see: project-types) When running release tests, what is your prefered branch name [build] Later you have the possibility to run so called release tests (NightlyBuilds) . Here you determine the branch name for the tests. The default here is build. Would you like to process changelogs during deployment [Y] dbFlow offers the possibility to generate changlogs based on the commit messages. Here you activate this function. ( see: changelog) What is the schema the changelog is processed with [schema_name] If you want changelogs to be displayed within your application, you can specify the target schema here, with which the corresponding TemplateCode should be executed. ( see: changelog) Enter database connections [localhost:1521/xepdb1] Place your connection string like: host:port/service Enter username of admin user (admin, sys, ...) [sys] This user is responsible for all scripts inside <code>db/_setup</code> folder Enter password for sys [leave blank and you will be asked for] Nothing more to clarify. Password is written to apply.env which is mentioned in .gitignore. (Keep in mind that passwords are saved obfuscated) Enter password for deployment_user (proxyuser: ?_depl) [leave blank and you will be asked for] Nothing more to clarify. Password is written to apply.env which is mentioned in .gitignore. (Keep in mind that passwords are saved obfuscated) Enter path to depot [_depot] This is a relative path which points to the depot (artifactory) and is also mentioned in .gitignore (see: depot) Enter stage of this configuration mapped to branch (develop, test, master) [develop] When importing the deployment, this setting assigns the database connection to the source branch Do you wish to generate and install default tooling? (Logger, utPLSQL, teplsql, tapi) [Y] Here you activate the initial installation of the different components/dependencies. These are placed in the <code>db/_setup/features</code> folder. There you can also place other features by yourself. If you don't need certain components, you can delete the corresponding file from the features folder (before running the actual installation).. Install with sql(cl) or sqlplus? [sqlplus] Here you define which CLI dbFLow should use to execute the SQL scripts. Enter path to place logfiles and artifacts into after installation? After a successful and also after an unsuccessful installation, all log files are stored in the folder specified here. Enter path to place logfiles into after installation? You can define a path where the log files will be stored as well. This is useful when you have instance repos and want to have a history of your deployments near by the instance itself. Enter application IDs (comma separated) you wish to use initialy (100,101,...) Here you can already enter the application IDs that dbFlow should initially take care of Enter restful Moduls (comma separated) you wish to use initialy (api,test,...) Here you can already specify the REST modules that dbFlow should initially take care of <p>After answering all the questions, your project structure is created with some SQL and bash files inside. You can now modifiy these files and / or put some files of your own into the corresponding folders.</p>"},{"location":"setup/#without-wizard","title":"Without wizard","text":"<p>Sometimes you want to have this questions and answers filled directly from the environment. In this case it is enough to specify the --wizard flag.</p> <pre><code>$ .dbFlow/setup.sh --generate &lt;project_name&gt; --wizard\n</code></pre> <p>This is especially useful if you want to build such a project scripted via a CI/CD.</p>"},{"location":"setup/#just-environment","title":"Just environment","text":"<p>This is especially important for instance directories. So if you assign a directory to a target stage, you can install the actual patch from depot and get the complete directory structure written to the current directory via this.</p> <pre><code>$ .dbFlow/setup.sh --generate &lt;project_name&gt; --envonly\n</code></pre>"},{"location":"setup/#copy-project","title":"Copy Project","text":"<p>As an addition to the generation of a project with the <code>--envonly</code> option, one has the possibility to copy the configuration and the setup, so the foundation, into a new target directory.</p> <pre><code>$ .dbFlow/setup.sh --copyto &lt;target-path&gt;\n</code></pre> <p>This is useful when you want to make your target instance project ready. So after customizing the data connection in the <code>apply.env</code> in the target directory you can install all dependencies like schemas, features and workspaces in the target instance.</p>"},{"location":"setup/#generating-applyenv-only","title":"Generating apply.env only","text":"<p>Mostly you will clone an existing dbFlow project and configure your environment to start working. In that case you just have to generate the apply.env file.</p> <pre><code>$ .dbFlow/setup.sh --apply\n</code></pre> <p>This will walk you through the wizard steps and outputs the file apply.env. So the fastest way to get into a dbFlow project would be the folloing snippet</p> <pre><code># clone the repo recursive and change into it\n$ git clone --recursive https://path-to-your-db-flow-project-where-dbFlow-is-allready-installed.git your_project &amp;&amp; cd your_project &amp;&amp; .dbFLow/setup.sh --apply\n</code></pre>"},{"location":"setup/#install-project","title":"Install Project","text":"<p>When you fullfilled the steps in the previous section, you are ready to install the project with dependencies into your database</p> <pre><code>$ .dbFlow/setup.sh --install\n</code></pre> <p>This will run all SQL and bash files inside the db/_setup directory.</p> <p>For deployment or release purposes. This is the time to make all branches equal. So if you are on master and this your only branch, create a branch for each deployment stage (develop, test, ...) or if you allready did that, just merge these changes into your branches.</p> <p>Important!</p> <p>The installation will abort if the users already exist in the DB as a schema. Furthermore, during the installation of the standard features, it will ask in each case, if they already exist, whether they should be overwritten. You have the option to flag this with a step force by using <code>.dbFlow/setup.sh --install --force</code>. In this case, all target schemas and features are deleted before the actual installation in each case.</p> <p>During the installation dbFlow will execute every SQL or bash file in alphabetical order. The sequence of folder will be:</p> <ol> <li>tablespaces</li> <li>directories</li> <li>users</li> <li>features</li> <li>workspaces</li> <li>acls</li> </ol>"},{"location":"start/","title":"Get Started with dbFLow","text":"<p>dbFlow is a deployment tool / framework for database driven applications in the oracle environment, mainly in the area of Oracle APEX. With dbFlow you can create and deploy multi-layered applications. dbFLow is powered by Git and can build a deployment / patch from different commit states, which can then be rolled out to different target instances.</p>"},{"location":"start/#prerequisite","title":"Prerequisite","text":"<ul> <li>dbFLow is written completely in bash and requires an appropriate environment for this.</li> <li>dbFlow uses Git to build the different releases and therefore requires a corresponding installation.</li> <li>dbFLow uses either SQLplus or SQLcl to build the releases into the target database. Therefore one of the two tools must be available.</li> </ul>"},{"location":"start/#get-dbflow","title":"Get dbFLow","text":"<p>dbFlow expects itself as a <code>.dbFlow</code> subdirectory in an existing main directory or Git repository. The best way to do this is to add dbFlow as a submodule in the repository.</p>"},{"location":"start/#1-create-a-git-repositoy","title":"1. Create a Git repositoy","text":"<pre><code>  $: mkdir demo &amp;&amp; cd demo &amp;&amp; git init\n</code></pre>"},{"location":"start/#2-add-dbflow-as-dbflow-submodule-to-your-project","title":"2. Add dbFlow as <code>.dbFlow</code> submodule to your Project","text":"<pre><code>  $: git submodule add https://github.com/MaikMichel/dbFlow.git .dbFlow\n</code></pre> <p>That's it. Now you have dbFlow successfully installed.</p>"},{"location":"start/#generate-a-dbflow-project","title":"Generate a dbFlow Project","text":""},{"location":"start/#3-run-setup-to-generate-a-project","title":"3. Run <code>setup</code> to generate a Project","text":"<pre><code>  $: .dbFlow/setup.sh --generate demo\n</code></pre>"},{"location":"start/#4-answer-some-question-based-on-your-requirements","title":"4. Answer some question based on your requirements","text":"<p>Info</p> <p>If you call <code>.dbFlow/setup.sh</code> without arguments you will see the possible options as a help page.</p>"},{"location":"start/#install-the-project-basement-and-dependencies","title":"Install the Project basement and dependencies","text":""},{"location":"start/#5-review-files-generated-for-you-and-make-some-adjustments","title":"5. Review files, generated for you and make some adjustments","text":"<p>dbFlow has created a very specific directory structure and a few scripts when you created the project. The scripts used for the installation can be found in the <code>db/_setup</code> folder. You can modify them according to your needs or add new ones.</p> <p>Info</p> <p>dbFlow comes with 4 features by default. These are OOS Logger, utPLSQL, tePLSQL and OM Tapigen. If you don't need these features, you can delete the scripts from the folder <code>db/_setup/features</code>.</p>"},{"location":"start/#6-run-setup-to-install-the-project","title":"6. Run <code>setup</code> to install the Project","text":"<pre><code>  $: .dbFlow/setup.sh --install\n</code></pre>"},{"location":"tutorial/","title":"A Tutorial","text":"<p>A Tutorial</p>"},{"location":"tutorial/#intro","title":"Intro","text":"<p>In this tutorial we will build a ToDo application. This application will be developed in our development database and will be updated from time to time in our production database. The production database reflects here only a part of our release pipeline. In a real environment we might need more target stages. For our tutorial this should be sufficient.</p> <p>To build the whole system locally, I use dockawex. Here, a complete development environment including a second build environment (this is our prod) is built via docker-compose. Absolutely recommended. See: dockawex</p>"},{"location":"tutorial/#prerequisites","title":"Prerequisites","text":"<ul> <li>Two database connections and the ability to connect as sys or another authorized user to create schemas and users.</li> <li>Every database with a proper installation of Oracle APEX</li> </ul> <p>just use dockawex and build and start your containers</p>"},{"location":"tutorial/#01-configuration","title":"01. Configuration","text":"<p>First we create a new project directory <code>todos</code>. In this directory we create the <code>_depot</code> folder, where the artifacts of the deployments will be stored and fetched later. In an <code>instance</code> directory we create a subfolder for each target environment. In our case this is the <code>prod</code> folder, which is the productive environment for this tutorial. Furthermore we create the actual working directory called <code>sources</code>. This is the actual development folder. and will be versioned via Git.</p> <pre><code># create master directory\n$: mkdir todos &amp;&amp; cd todos\n\n# create folder to hold depot, target-instance and sources\ntodos$: mkdir _depot &amp;&amp; mkdir -p instances/prod &amp;&amp; mkdir sources\n\n# just to show current content\ntodos$: find\n\n# should produce\n# .\n# ./_depot\n# ./instances\n# ./instances/prod\n# ./sources\n</code></pre> <p>Now we change to our actual source directory, turn on version control and install dbFlow.</p> <pre><code># go to sources\ntodos$: cd sources\n\n# add git\nsources$: curl -sS https://raw.githubusercontent.com/MaikMichel/dbFlow/master/install.sh | bash\n</code></pre> <p>Now that we have submitted the directory, we can save the current empty state and switch to a new develop branch where we will work.</p> <pre><code># add current changes\nsources[master]$: git add .\n\n# do the commit\nsources[master]$: git commit -m \"Inital commit, with dbFlow added as submodule\"\n\n# create new branch and jump right in\nsources[master]$: git checkout -b develop\n</code></pre>"},{"location":"tutorial/#02-setup-dbflow","title":"02. Setup dbFlow","text":"<p>Now that we have the directory and git set up, we can configure dbFlow. For our application and just for demo purposes a SingleSchema is completely sufficient. Additionally we place the depot a level up and negate the installation of the default features. I leave all other settings in the default.</p> <pre><code>sources[develop]$: .dbFlow/setup.sh --generate todo\n</code></pre> <p>output of project wizard</p> <pre><code>Generate Project: todo\nWhich dbFLow project type do you want to create? Single, Multi or Flex [M]: S\nWhen running release tests, what is your prefered branch name [build]:\nWould you like to process changelogs during deployment [N]:\nEnter database connections [localhost:1521/xepdb1]:\nEnter username of admin user (admin, sys, ...) [sys]:\nEnter password for sys [leave blank and you will be asked for]: ************\nEnter password for user todo [leave blank and you will be asked for]: ************\nEnter path to depot [_depot]: ../_depot\nEnter stage of this configuration mapped to branch (develop, test, master) [develop]:\nDo you wish to generate and install default tooling? (Logger, utPLSQL, teplsql, tapi) [Y]: N\nInstall with sql(cl) or sqlplus? [sqlplus]:\nEnter path to place logfiles and artifacts into after installation? [_log_path]\nEnter application IDs (comma separated) you wish to use initialy (100,101,...):\nEnter restful Moduls (comma separated) you wish to use initialy (api,test,...):\n</code></pre> <p>output of dbFlow, when generating the project</p> <pre><code>Generating project with following options\n  Project:                          todo\n  Mode:                             S\n  Build Branch:                     build\n  Create Changelos:                 N\n  Schema Changelog proccessed:\n  Connection:                       localhost:1521/xepdb1\n  Admin User:                       sys\n  Deployment User:                  todo\n  Location depot:                   ../_depot\n  Branch is mapped to Stage:        develop\n  SQl commandline:                  sqlplus\n  Install default tools:            N\n  Configure with default apps:\n  Configure with default modules:\n  Just install environment onyl:    NO\n\n... workinging ...\n\n\nCongratulations!\nYour project todo has been successfully created.\nScripts have been added inside directory: db/_setup that allow you\nto create the respective schemas, workspaces as well as ACLs and features, as long\nas you specified them during the configuration.\n\ntodo - directory structure\n|-- _depot                 &gt;&gt; Path to store your build artifacts\n|-- .dbFlow                &gt;&gt; dbFlow itself\n|-- .hooks                 &gt;&gt; Scripts/Tasks to run pre or post deployment\n|-- apex                   &gt;&gt; APEX applications in subfolders (f123)\n|-- db                     &gt;&gt; All DB Schemas used\n|   |-- _setup             &gt;&gt; Scripts to create schemas, features, workspaces, ...\n|   |-- .hooks             &gt;&gt; Scripts/Tasks to run pre or post db schema deployments\n|   |-- todo               &gt;&gt; Main DB Schema mostly used for SingleMode\n|-- reports                &gt;&gt; Place all your binaries for upload in a seperate folder here\n|-- rest                   &gt;&gt; REST Modules\n|   |-- access             &gt;&gt; Place all your privileges, roles and clients here (plsql)\n|   |-- modules            &gt;&gt; The REST modules inside seperate folders\n|-- static                 &gt;&gt; StaticFiles used to uploads go here (managed by dbFlux)\napply.env                  &gt;&gt; Environment configuration added to .gitignore\nbuild.env                  &gt;&gt; Project configuration\n</code></pre>"},{"location":"tutorial/#review-folders","title":"Review folders","text":"<p>dbFLow has now created the complete directory structure for us. Additionally we see the files <code>build.env</code> and <code>apply.env</code>. In the file <code>build.env</code> project specific settings are stored and in the file <code>apply.env</code> environment specific settings are stored. Therefore dbFlow put the apply.env directly on the <code>gitignore</code> list. Later, when it comes to deployment to another environment, this configuration is adapted to the target system accordingly.</p> <p>dbFlow creates a folder <code>db/_setup</code>. Here we will find the scripts necessary to create the schema and the workspace for APEX. All files can be edited as required. dbFlow will install all existing files in the appropriate order. See: install project</p>"},{"location":"tutorial/#03-install-project-definition-setup-to-database","title":"03. Install project definition (setup) to database","text":"<p>Now that the project structure has been successfully created and we have the import scripts reviewed in the setup folder, we can import the project to the database</p> <pre><code>sources[develop]$: .dbFlow/setup.sh --install\n</code></pre> <p>dbFlow will now execute all scripts.</p> <p>Important</p> <p>Remember, dbFlow will delete the target schema on its own and create it again. If the target schema already exists, dbFlow will abort the installation. Only the <code>--force</code> option can be used to force this behavior.</p> <p>When everything is done, the actual work can begin. In our case, of course, we honor this with a corresponding commit.</p> <pre><code>sources[develop]$: git add . &amp;&amp; git commit -m \"dbFlow Project created and applied to dev\"\n</code></pre>"},{"location":"tutorial/#04-implement-the-first-version-of-our-todo-app","title":"04. Implement the first version of our todo app","text":"<p>I won't go to much into the details. All code fragments are meant to show you the working process with dbFlow as deployment tool and framework.</p>"},{"location":"tutorial/#download-and-unpack-files-of-version-1","title":"Download and unpack files of version 1","text":"<pre><code># Download Release 1\nsources[develop]$: curl -o release_1.tar.gz -L https://github.com/MaikMichel/dbFlow-demo-todoapp/archive/refs/tags/1.0.0.tar.gz\n\n# Unpack Application f108 to apex\nsources[develop]$: tar -xzvf release_1.tar.gz --directory=\"apex\" \"dbFlow-demo-todoapp-1.0.0/apex\" --strip-components=2\n\n# Unpack Schema files to db/todo\nsources[develop]$: tar -xzvf release_1.tar.gz --directory=\"db/todo\" \"dbFlow-demo-todoapp-1.0.0/db/todo\" --strip-components=3\n\n# Remove downloaded release_1.tar.gz\nrm release_1.tar.gz\n</code></pre> <p>Note</p> <p>If you want to install the demo application with another ID then 108 you can rename the folder as you like (f###)</p>"},{"location":"tutorial/#install-application","title":"Install Application","text":"<p>You have downloaded all the files and unpacked them into your prepared dbFlow project. That means we can tell dbFLow to install all the files. All you have to do is to build an initial version and give it the version name \"install\". dbFlow will then directly install the freshly built version into the current environment.</p> <pre><code>sources[develop]$: .dbFlow/build.sh --init --version install\n</code></pre> <p>Then you can familiarize yourself with the application and the files it contains. In the schema you will find the tasks table and corresponding constraints, indexes and triggers. Additionally you will find the package apx_tasks_util, which contains the business logic of our application.</p> <p></p> <p>When you go to your APEX instance, you can log in with the default user <code>wsadmin</code>. When you log in for the first time, you will have to enter a new password. The initial password is always the username. Unless you have changed it before installing the setup. See file: <code>db/_setup/workspaces/todo/create_01_user_wsadmin.sql</code></p> <p> </p> <p>The application is a simple task management. </p> <p>Now that we have our first version installed, we commit our current state.</p> <pre><code>sources[develop]$: git add .\nsources[develop]$: git commit -m \"Version 1 of ToDo App\"\n</code></pre>"},{"location":"tutorial/#deploy-application","title":"Deploy Application","text":"<p>Now we have finished version 1 and want to deploy it to the production environment right away. To do this, we are now building our first deployment, our initial patch. The concept, which dbFlow is based on, assumes that there is a corresponding branch per target environment. (See: Concept)</p> <p>Therefore we now merge our current state in the branch <code>develop</code> to the <code>master</code> branch, our target branch.</p> <pre><code>sources[develop]$: git checkout master\nsources[master]$: git merge develop\n</code></pre> <p>Now we create the initial deployment for our production environment.</p> <pre><code>sources[master]$: .dbFlow/build.sh --init --version 1.0.0\n</code></pre> <p>As soon as the build of an artifact is executed by the master branch, dbFlow will ask if a tag with the corresponding version should be committed. In our case we can skip that by entering <code>N</code>. dbFlow will now place the deployment artifact in the master folder in the depot.</p> <p></p> <p>The target depot is always a sub directory with the same name as the branch from which the artifact is created.</p>"},{"location":"tutorial/#05-prepare-target-instance","title":"05. Prepare Target Instance","text":"<p>The target instances in this tutorial are all accessible from development. In a real world scenario, they will be located on other hosts.</p> <p>In the preparations we have already stored our directory structure. Now we want to distribute our current application to the target instance.</p>"},{"location":"tutorial/#copy-configuration","title":"Copy Configuration","text":"<p>In this step we copy the current configuration to the instance directory.</p> <pre><code>sources[master]$: .dbFlow/setup.sh --copyto \"../instances/prod\"\n</code></pre> <p>dbFlow will now copy the <code>db/_setup</code> directory and the <code>build.env</code> and <code>apply.env</code> files to the target directory. Afterwards dbFlow will also add itself as a submodule in this directory. Let's go to the instance directory and modify some configurations.</p> <pre><code>sources[master]$: cd ../instances/prod\ninstances/prod[master]$: code apply.env\n</code></pre> <p>I use VisualStudio Code to edit the file <code>apply.env</code> but you could any other editor too.</p>"},{"location":"tutorial/#edit-tns","title":"Edit TNS","text":"<p>Since the copy of the configuration of course still points to the same database environment, it must now be adapted. For this we change the corresponding entry in the file <code>../instances/prod/apply.env</code>.</p> <p></p> <p>In our case, this is now the xepdb2 instead of xepdb1.</p>"},{"location":"tutorial/#edit-stage","title":"Edit Stage","text":"<p>Additionally we adjust the entry <code>STAGE</code>, because this points to the branch folder in the depot, which corresponds to our target stage. All artifacts that dbFlow imports are expected here. Unless you use the <code>--noextract</code> option when importing.</p> <p></p>"},{"location":"tutorial/#edit-depot","title":"Edit Depot","text":"<p>Because we have an additional level in the directory tree, we also have to adjust the path to the depot and change it from <code>../_depot</code> to <code>../../_depot</code>.</p> <p></p>"},{"location":"tutorial/#install-project-definition-setup-to-target-database","title":"Install project definition (setup) to target database","text":"<p>Now we can install the setup meaning our foundation of the application on the target database.</p> <pre><code>instances/prod[master]$: .dbFlow/setup.sh --install\n</code></pre>"},{"location":"tutorial/#06-install-first-version-to-target-instance","title":"06. Install first version to target instance","text":"<p>Now that we have everything set up, we can simply deploy the previously deployed artifact to the target instance.</p> <pre><code>instances/prod[master]$: .dbFlow/apply.sh --init --version 1.0.0\n</code></pre> <p>Since we want to do an <code>init</code> here, dbFlow will ask you if you really want to do this, cause an <code>init</code> will ALWAYS make all target schemas empty (delete all objects). For a CI/CD environment you can prevent this by setting the environment variable DBFLOW_JENKINS=\"YES\".</p> <p>Now you should be able to visit our ToDo Application on your target instance.</p> <p>Because the target instance folder is also a Git folder, we can now commit everything to keep a proper history of our deployments in to that stage.</p> <pre><code>instances/prod[master]$: git add . &amp;&amp; git commit -m \"1.0.0\"\n</code></pre>"},{"location":"tutorial/#07-implement-the-next-version-of-our-todo-app","title":"07. Implement the next version of our todo app","text":"<p>We will have to go to our development folder and checkout the develop branch.</p> <pre><code>instances/prod[master]$: cd ../../sources\nsources[master]$: git checkout develop\nsources[develop]$:\n</code></pre>"},{"location":"tutorial/#download-and-unpack-files-of-version-2","title":"Download and unpack files of version 2","text":"<pre><code># Download Release 2\nsources[develop]$: curl -o release_2.tar.gz -L https://github.com/MaikMichel/dbFlow-demo-todoapp/archive/refs/tags/1.1.0.tar.gz\n\n# Unpack Application f108 to apex\nsources[develop]$: tar -xzvf release_2.tar.gz --directory=\"apex\" \"dbFlow-demo-todoapp-1.1.0/apex\" --strip-components=2\n\n# Unpack Schema files to db/todo\nsources[develop]$: tar -xzvf release_2.tar.gz --directory=\"db/todo\" \"dbFlow-demo-todoapp-1.1.0/db/todo\" --strip-components=3\n\n# Remove downloaded release_2.tar.gz\nrm release_2.tar.gz\n</code></pre> <p>Note</p> <p>If you want to install the demo application with another ID then 108 you can rename the folder as you like (f###). Keep in mind, that in the case you have to remove the previous application folder.</p>"},{"location":"tutorial/#install-changes-to-development-environment","title":"Install Changes to development environment","text":"<p>After you extracted the changes from version 2 of the app you should have see the following changes.</p> <p></p> <p>Let's add them an commit the changes.</p> <pre><code>sources[develop]$: git add .\n</code></pre> <p>You can ignore the version.md file here. This file will allways be created when applying an artifact. Normaly this would never happen in development. This is just because we installed the sources, downloaded from github, as patches.</p> <p>Now install changed files to develop environment by building a patch and installing it instantly.</p> <pre><code>sources[develop]$: .dbFlow/build.sh --patch --version install --cached\n</code></pre> <p>Remember, when using \"<code>install</code>\" as version identifier dbFlow will apply the patch immediatly.</p> <p>Note</p> <p>The flag <code>--cached</code> tells dbFlow to include only the staged files. This is a suiteable hint just for your development environment. This is about installing the extracted files of the zip directly.</p> <p>We will commit the changes in order to be ready to build the first <code>patch</code> for out target environment.</p> <pre><code>sources[develop]$: git add . &amp;&amp; git commit -m \"Version 2 of ToDo App\"\n</code></pre>"},{"location":"tutorial/#install-changes-to-production-environment","title":"Install Changes to production environment","text":"<p>To deploy an artifact to the production environment, we have to merge the changes to our target branch: master.</p> <pre><code>sources[develop]$: git checkout master\nsources[master]$: git merge develop\n</code></pre> <p>Now create the patch itself</p> <pre><code>sources[master]$: .dbFlow/build.sh --patch --version 1.1.0\n</code></pre> <p>dbFlow will ask you to tag this specific version. This happens, when you build on main or master branch. You can ignore this at this moment. Keep in mind, that you can turn this off whenn running inside a CI/CD environment.</p> <p>dbFlow has now shipped the patch to the depot (<code>../_depot/master</code>).</p> <p>To apply the patch to production we just have to go to the predefined instance directory and apply the patch.</p> <pre><code>sources[master]$: cd ../instances/prod\ninstances/prod[master]$: .dbFlow/apply.sh --patch --version 1.1.0\n</code></pre> <p>Thats it. After that, your production instance should reflect the current changes and display the version on the footer.</p>"}]}